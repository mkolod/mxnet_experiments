{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-18 00:03:18.124153. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from collections import namedtuple, Counter\n",
    "from unidecode import unidecode\n",
    "from mxnet.io import DataIter\n",
    "\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-18 00:03:18.352541. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "Args = namedtuple(\n",
    "    'Args', \n",
    "    ('test load_epoch num_layers num_hidden num_embed bidirectional gpus '\n",
    "     'kv_store num_epochs optimizer mom wd lr batch_size disp_batches '\n",
    "     'stack_rnn dropout model_prefix'))\n",
    "\n",
    "args = Args(\n",
    "    test=          False,\n",
    "    load_epoch=    0,\n",
    "    num_layers=    2,\n",
    "    num_hidden=    200,\n",
    "    num_embed=     200,\n",
    "    bidirectional= False,\n",
    "    gpus=          '0,1',\n",
    "    kv_store=      'device',\n",
    "    num_epochs=    1,\n",
    "    optimizer=    'adam',\n",
    "    mom=           0.9,\n",
    "    wd=           0.00001,\n",
    "    lr = 0.001,\n",
    "    batch_size=    32,\n",
    "    disp_batches=  50,\n",
    "    stack_rnn=     False,\n",
    "    dropout=       0.5,\n",
    "    model_prefix= 'foo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-18 00:03:18.938311. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# Do a word count to get the number of words \n",
    "\n",
    "start_label = 1\n",
    "invalid_label = 0\n",
    "\n",
    "# Decode text as UTF-8\n",
    "# Remove diacritical signs and convert to Latin alphabet\n",
    "# Separate punctuation as separate \"words\"\n",
    "def tokenize_text(fname, vocab=None, invalid_label=-1, start_label=0, sep_punctuation=True):\n",
    "    lines = unidecode(open(fname).read().decode('utf-8')).split('\\n')\n",
    "    lines = [x for x in lines if x]\n",
    "    lines = map(lambda x: re.findall(r\"\\w+|[^\\w\\s]\", x, re.UNICODE), lines)    \n",
    "    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label, start_label=start_label)\n",
    "    return sentences, vocab\n",
    "\n",
    "def get_data(src_path, targ_path, start_label=1, invalid_label=0):\n",
    "    src_sent, src_vocab = tokenize_text(src_path, start_label=start_label,\n",
    "                                invalid_label=invalid_label)\n",
    "    \n",
    "    new_start = max(src_vocab.values())\n",
    "    \n",
    "    targ_sent, targ_vocab = tokenize_text(targ_path, start_label=new_start+1,\n",
    "                                          invalid_label=invalid_label)\n",
    "\n",
    "    return src_sent, src_vocab, targ_sent, targ_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-18 00:03:20.193902. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "src_sent, src_vocab, targ_sent, targ_vocab = \\\n",
    "    get_data(\n",
    "        src_path='./data/europarl-v7.pl-en.small.en', \n",
    "        targ_path='./data/europarl-v7.pl-en.small.pl',\n",
    "        start_label=1,\n",
    "        invalid_label=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-18 00:03:21.322344. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "def gen_buckets(src_sent, targ_sent, filter_smaller_counts_than=None):\n",
    "    length_pairs = map(lambda x: (len(x[0]), len(x[1])), zip(src_sent, targ_sent))\n",
    "    counts = list(Counter(length_pairs).items())\n",
    "    c_sorted = sorted(counts, key=operator.itemgetter(0, 1))\n",
    "    buckets = [i for i in c_sorted if i[1] >= filter_smaller_counts_than]\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-18 00:03:22.353586. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "all_buckets = gen_buckets(src_sent, targ_sent, filter_smaller_counts_than=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-18 00:03:25.817274. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "bucket_filter = 32\n",
    "batched_buckets = gen_buckets(src_sent, targ_sent, filter_smaller_counts_than=bucket_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of all buckets: 163 (# sent: 1000)\n",
      "# of buckets with counts < 32 filtered out: 4 (num sent: 371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-18 00:03:26.473909. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "num_sent = lambda sent: reduce(lambda a, b: a + b, map(lambda x: x[1], sent))\n",
    "num_all = num_sent(all_buckets)\n",
    "num_batched = num_sent(batched_buckets)\n",
    "print(\"# of all buckets: %d (# sent: %d)\" % (len(all_buckets), num_all))\n",
    "print(\"# of buckets with counts < %d filtered out: %d (num sent: %d)\" % (bucket_filter, len(batched_buckets), num_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "filter expected 2 arguments, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-54d11a897db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: filter expected 2 arguments, got 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-20 17:20:36.807798. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "filter(zip(src_sent, targ_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_sorted_buckets(buckets):\n",
    "    b = sorted(buckets, key=operator.itemgetter(0, 1))\n",
    "    for i in b:\n",
    "        print(i)\n",
    "\n",
    "print_sorted_buckets(batched_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1) Filter out sentence pairs of which there are fewer than the batch size?\n",
    "# 2) Use pairs from 0 to 60 in increments of 5?\n",
    "# 3) Create buckets only based on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoDBisect:\n",
    "    def __init__(self, buckets):\n",
    "        self.buckets = sorted(buckets, key=operator.itemgetter(0, 1))\n",
    "        self.x, self.y = zip(*buckets)\n",
    "        self.x, self.y = np.array(list(self.x)), np.array(list(self.y))\n",
    "        \n",
    "    def twod_bisect(self, source, target):    \n",
    "        offset1 = np.searchsorted(self.x, len(source), side='left')\n",
    "        offset2 = np.where(self.y[offset1:] >= len(target))[0]        \n",
    "        return self.buckets[offset2[0]]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BucketSeq2SeqIter(DataIter):\n",
    "    \"\"\"Simple bucketing iterator for sequence-to-sequence models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list of list of pairs of int\n",
    "        encoded sentences (source seq dict int, target seq dict int)\n",
    "    batch_size : int\n",
    "        batch_size of data    \n",
    "    invalid_label : int, default -1\n",
    "        key for invalid label, e.g. <end-of-sentence>\n",
    "    dtype : str, default 'float32'\n",
    "        data type\n",
    "    buckets : list of pairs of int (source seq length, target seq length)\n",
    "        size of data buckets. Automatically generated if None.\n",
    "    data_name : str, default 'data'\n",
    "        name of data    print(buckets)\n",
    "    label_name : str, default 'softmax_label'\n",
    "        name of label\n",
    "    layout : str\n",
    "        format of data and label. 'NT' means (batch_size, length)\n",
    "        and 'TN' means (length, batch_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_seq, output_seq, batch_size, buckets=None, invalid_label=-1,\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "                 layout='NTC'):\n",
    "        super(BucketSeq2SeqIter, self).__init__()\n",
    "        \n",
    "        # insert call to gen_buckets here if buckets are not provided\n",
    "        if not buckets:\n",
    "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\n",
    "                       if j >= batch_size]\n",
    "        # Sorting is kinda pointless because it's first by the first element of the tuple,\n",
    "        # then the next. So, it could be [(1, 2), (1, 20), (2, 5), (2, 71)]\n",
    "        buckets.sort()\n",
    "\n",
    "        ndiscard = 0\n",
    "        self.data = [[] for _ in buckets]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            \n",
    "            # this bisect also won't work because it's now based on a tuple of lengths\n",
    "            buck = bisect.bisect_left(buckets, len(sent))\n",
    "            # this test is not appropriate because of the tuple of lengths\n",
    "            if buck == len(buckets):\n",
    "                ndiscard += 1\n",
    "                continue\n",
    "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
    "            buff[:len(sent)] = sent\n",
    "            self.data[buck].append(buff)\n",
    "\n",
    "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\n",
    "\n",
    "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.invalid_label = invalid_label\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        self.major_axis = layout.find('N')\n",
    "        self.default_bucket_key = max(buckets)\n",
    "\n",
    "        if self.major_axis == 0:\n",
    "            self.provide_data = [(data_name, (batch_size, self.default_bucket_key))]\n",
    "            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]\n",
    "        elif self.major_axis == 1:\n",
    "            self.provide_data = [(data_name, (self.default_bucket_key, batch_size))]\n",
    "            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\n",
    "\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.data):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.idx)\n",
    "        for buck in self.data:\n",
    "            np.random.shuffle(buck)\n",
    "\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        for buck in self.data:\n",
    "            label = np.empty_like(buck)\n",
    "            label[:, :-1] = buck[:, 1:]\n",
    "            label[:, -1] = self.invalid_label\n",
    "            self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        if self.major_axis == 1:\n",
    "            data = self.nddata[i][j:j+self.batch_size].T\n",
    "            label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "        else:\n",
    "            data = self.nddata[i][j:j+self.batch_size]\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "        return DataBatch([data], [label],\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[(self.data_name, data.shape)],\n",
    "                         provide_label=[(self.label_name, label.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(batched_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buckets = list(range(5, 60, 5))\n",
    "#[10, 20, 30, 40, 50, 60]\n",
    "\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "start_label = 1\n",
    "invalid_label = 0\n",
    "\n",
    "def tokenize_text(fname, vocab=None, invalid_label=-1, start_label=0):\n",
    "    lines = open(fname).readlines()\n",
    "    lines = [filter(None, i.split(' ')) for i in lines]\n",
    "    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label, start_label=start_label)\n",
    "    return sentences, vocab\n",
    "\n",
    "def get_data(layout):\n",
    "    train_sent, vocab = tokenize_text(\"./data/ptb.train.txt\", start_label=start_label,\n",
    "                                      invalid_label=invalid_label)\n",
    "    val_sent, _ = tokenize_text(\"./data/ptb.test.txt\", vocab=vocab, start_label=start_label,\n",
    "                                invalid_label=invalid_label)\n",
    "\n",
    "    data_train  = mx.rnn.BucketSentenceIter(train_sent, args.batch_size, buckets=buckets,\n",
    "                                            invalid_label=invalid_label, layout=layout)\n",
    "    data_val    = mx.rnn.BucketSentenceIter(val_sent, args.batch_size, buckets=buckets,\n",
    "                                            invalid_label=invalid_label, layout=layout)\n",
    "    \n",
    "    print(\"default: %s\" % data_train.default_bucket_key)\n",
    "    return data_train, data_val, vocab\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    data_train, data_val, vocab = get_data('TN')\n",
    "    if args.stack_rnn:\n",
    "        stack = mx.rnn.SequentialRNNCell()\n",
    "        for layer in range(args.num_layers):\n",
    "            dropout = 0.0\n",
    "            if layer < (args.num_layers - 1):\n",
    "                dropout = args.dropout\n",
    "            stack.add(mx.rnn.FusedRNNCell(args.num_hidden, num_layers=1,\n",
    "                    mode='lstm', prefix='lstm_%d'%layer, dropout=dropout,\n",
    "                    bidirectional=args.bidirectional))\n",
    "        cell = stack\n",
    "    else:\n",
    "        cell = mx.rnn.FusedRNNCell(args.num_hidden, num_layers=args.num_layers, dropout=args.dropout,\n",
    "                mode='lstm', bidirectional=args.bidirectional)\n",
    "\n",
    "    def sym_gen(seq_len):\n",
    "        data = mx.sym.Variable('data')\n",
    "        label = mx.sym.Variable('softmax_label')\n",
    "        embed = mx.sym.Embedding(data=data, input_dim=len(vocab), output_dim=args.num_embed,name='embed')\n",
    "\n",
    "        output, _ = cell.unroll(seq_len, inputs=embed, merge_outputs=True, layout='TNC')\n",
    "\n",
    "        pred = mx.sym.Reshape(output,\n",
    "                shape=(-1, args.num_hidden*(1+args.bidirectional)))\n",
    "        pred = mx.sym.FullyConnected(data=pred, num_hidden=len(vocab), name='pred')\n",
    "\n",
    "        label = mx.sym.Reshape(label, shape=(-1,))\n",
    "        pred = mx.sym.SoftmaxOutput(data=pred, label=label, name='softmax')\n",
    "\n",
    "        return pred, ('data',), ('softmax_label',)\n",
    "\n",
    "    if args.gpus:\n",
    "        contexts = [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "    else:\n",
    "        contexts = mx.cpu(0)\n",
    "\n",
    "    model = mx.mod.BucketingModule(\n",
    "        sym_gen             = sym_gen,\n",
    "        default_bucket_key  = data_train.default_bucket_key,\n",
    "        context             = contexts)\n",
    "\n",
    "    if args.load_epoch:\n",
    "        _, arg_params, aux_params = mx.rnn.load_rnn_checkpoint(\n",
    "            cell, args.model_prefix, args.load_epoch)\n",
    "    else:\n",
    "        arg_params = None\n",
    "        aux_params = None\n",
    "\n",
    "    opt_params = {\n",
    "      'learning_rate': args.lr,\n",
    "      'wd': args.wd\n",
    "    }\n",
    "\n",
    "    if args.optimizer not in ['adadelta', 'adagrad', 'adam', 'rmsprop']:\n",
    "        opt_params['momentum'] = args.mom\n",
    "\n",
    "    model.fit(\n",
    "        train_data          = data_train,\n",
    "        eval_data           = data_val,\n",
    "        eval_metric         = mx.metric.Perplexity(invalid_label),\n",
    "        kvstore             = args.kv_store,\n",
    "        optimizer           = args.optimizer,\n",
    "        optimizer_params    = opt_params, \n",
    "        initializer         = mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n",
    "        arg_params          = arg_params,\n",
    "        aux_params          = aux_params,\n",
    "        begin_epoch         = args.load_epoch,\n",
    "        num_epoch           = args.num_epochs,\n",
    "        batch_end_callback  = mx.callback.Speedometer(args.batch_size, args.disp_batches),\n",
    "        epoch_end_callback  = mx.rnn.do_rnn_checkpoint(cell, args.model_prefix, 1)\n",
    "                              if args.model_prefix else None)\n",
    "\n",
    "def test(args):\n",
    "    assert args.model_prefix, \"Must specifiy path to load from\"\n",
    "    _, data_val, vocab = get_data('NT')\n",
    "\n",
    "    if not args.stack_rnn:\n",
    "        stack = mx.rnn.FusedRNNCell(args.num_hidden, num_layers=args.num_layers,\n",
    "                mode='lstm', bidirectional=args.bidirectionclass BucketSentenceIter(DataIter):\n",
    "    \"\"\"Simple bucketing iterator for language model.\n",
    "    Label for each step is constructed from data of\n",
    "    next step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list of list of int\n",
    "        encoded sentences\n",
    "    batch_size : int\n",
    "        batch_size of data\n",
    "    invalid_label : int, default -1\n",
    "        key for invalid label, e.g. <end-of-sentence>\n",
    "    dtype : str, default 'float32'\n",
    "        data type\n",
    "    buckets : list of int\n",
    "        size of data buckets. Automatically generated if None.\n",
    "    data_name : str, default 'data'\n",
    "        name of data\n",
    "    label_name : str, default 'softmax_label'\n",
    "        name of label\n",
    "    layout : str\n",
    "        format of data and label. 'NT' means (batch_size, length)\n",
    "        and 'TN' means (length, batch_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences, batch_size, buckets=None, invalid_label=-1,\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "                 layout='NTC'):\n",
    "        super(BucketSentenceIter, self).__init__()\n",
    "        if not buckets:\n",
    "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\n",
    "                       if j >= batch_size]\n",
    "        buckets.sort()\n",
    "\n",
    "        ndiscard = 0\n",
    "        self.data = [[] for _ in buckets]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            buck = bisect.bisect_left(buckets, len(sent))\n",
    "            if buck == len(buckets):\n",
    "                ndiscard += 1\n",
    "                continue\n",
    "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
    "            buff[:len(sent)] = sent\n",
    "            self.data[buck].append(buff)\n",
    "\n",
    "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\n",
    "\n",
    "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.invalid_label = invalid_label\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        self.major_axis = layout.find('N')\n",
    "        self.default_bucket_key = max(buckets)\n",
    "\n",
    "        if self.major_axis == 0:\n",
    "            self.provide_data = [(data_name, (batch_size, self.default_bucket_key))]\n",
    "            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]\n",
    "        elif self.major_axis == 1:\n",
    "            self.provide_data = [(data_name, (self.default_bucket_key, batch_size))]\n",
    "            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\n",
    "\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.data):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.idx)\n",
    "        for buck in self.data:\n",
    "            np.random.shuffle(buck)\n",
    "\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        for buck in self.data:\n",
    "            label = np.empty_like(buck)\n",
    "            label[:, :-1] = buck[:, 1:]\n",
    "            label[:, -1] = self.invalid_label\n",
    "            self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        if self.major_axis == 1:\n",
    "            data = self.nddata[i][j:j+self.batch_size].T\n",
    "            label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "        else:\n",
    "            data = self.nddata[i][j:j+self.batch_size]\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "        return DataBatch([data], [label],\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[(self.data_name, data.shape)],\n",
    "                         provide_label=[(self.label_name, label.shape)])al).unfuse()\n",
    "    else:\n",
    "        stack = mx.rnn.SequentialRNNCell()\n",
    "        for i in range(args.num_layers):\n",
    "            cell = mx.rnn.LSTMCell(num_hidden=args.num_hidden, prefix='lstm_%dl0_'%i)\n",
    "            if args.bidirectional:\n",
    "                cell = mx.rnn.BidirectionalCell(\n",
    "                        cell,\n",
    "                        mx.rnn.LSTMCell(num_hidden=args.num_hidden, prefix='lstm_%dr0_'%i),\n",
    "                        output_prefix='bi_lstm_%d'%i)\n",
    "            stack.add(cell)\n",
    "\n",
    "    def sym_gen(seq_len):\n",
    "        data = mx.sym.Variable('data')\n",
    "        label = mx.sym.Variable('softmax_label')\n",
    "        embed = mx.sym.Embedding(data=data, input_dim=len(vocab),\n",
    "                                 output_dim=args.num_embed, name='embed')\n",
    "\n",
    "        stack.reset()\n",
    "        outputs, states = stack.unroll(seq_len, inputs=embed, merge_outputs=True)\n",
    "\n",
    "        pred = mx.sym.Reshape(outputs,\n",
    "                shape=(-1, args.num_hidden*(1+args.bidirectional)))\n",
    "        pred = mx.sym.FullyConnected(data=pred, num_hidden=len(vocab), name='pred')\n",
    "\n",
    "        label = mx.sym.Reshape(label, shape=(-1,))\n",
    "        pred = mx.sym.SoftmaxOutput(data=pred, label=label, name='softmax')\n",
    "\n",
    "        return pred, ('data',), ('softmax_label',)class BucketSentenceIter(DataIter):\n",
    "    \"\"\"Simple bucketing iterator for language model.\n",
    "    Label for each step is constructed from data of\n",
    "    next step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list of list of int\n",
    "        encoded sentences\n",
    "    batch_size : int\n",
    "        batch_size of data\n",
    "    invalid_label : int, default -1\n",
    "        key for invalid label, e.g. <end-of-sentence>\n",
    "    dtype : str, default 'float32'\n",
    "        data type\n",
    "    buckets : list of int\n",
    "        size of data buckets. Automatically generated if None.\n",
    "    data_name : str, default 'data'\n",
    "        name of data\n",
    "    label_name : str, default 'softmax_label'\n",
    "        name of label\n",
    "    layout : str\n",
    "        format of data and label. 'NT' means (batch_size, length)\n",
    "        and 'TN' means (length, batch_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences, batch_size, buckets=None, invalid_label=-1,\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "                 layout='NTC'):\n",
    "        super(BucketSentenceIter, self).__init__()\n",
    "        if not buckets:\n",
    "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\n",
    "                       if j >= batch_size]\n",
    "        buckets.sort()\n",
    "\n",
    "        ndiscard = 0\n",
    "        self.data = [[] for _ in buckets]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            buck = bisect.bisect_left(buckets, len(sent))\n",
    "            if buck == len(buckets):\n",
    "                ndiscard += 1\n",
    "                continue\n",
    "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
    "            buff[:len(sent)] = sent\n",
    "            self.data[buck].append(buff)\n",
    "\n",
    "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\n",
    "\n",
    "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.invalid_label = invalid_label\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        self.major_axis = layout.find('N')\n",
    "        self.default_bucket_key = max(buckets)\n",
    "\n",
    "        if self.major_axis == 0:\n",
    "            self.provide_data = [(data_name, (batch_size, self.default_bucket_key))]\n",
    "            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]\n",
    "        elif self.major_axis == 1:\n",
    "            self.provide_data = [(data_name, (self.default_bucket_key, batch_size))]\n",
    "            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\n",
    "\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.data):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.idx)\n",
    "        for buck in self.data:\n",
    "            np.random.shuffle(buck)\n",
    "\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        for buck in self.data:\n",
    "            label = np.empty_like(buck)\n",
    "            label[:, :-1] = buck[:, 1:]\n",
    "            label[:, -1] = self.invalid_label\n",
    "            self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        if self.major_axis == 1:\n",
    "            data = self.nddata[i][j:j+self.batch_size].T\n",
    "            label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "        else:\n",
    "            data = self.nddata[i][j:j+self.batch_size]\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "        return DataBatch([data], [label],\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[(self.data_name, data.shape)],\n",
    "                         provide_label=[(self.label_name, label.shape)])\n",
    "\n",
    "    if args.gpus:\n",
    "        contexts = [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "    else:\n",
    "        contexts = mx.cpu(0)\n",
    "\n",
    "    model = mx.mod.BucketingModule(\n",
    "        sym_gen             = sym_gen,\n",
    "        default_bucket_key  = data_val.default_bucket_key,\n",
    "        context             = contexts)\n",
    "    model.bind(data_val.provide_data, data_val.provide_label, for_training=False)\n",
    "\n",
    "    # note here we load using SequentialRNNCell instead of FusedRNNCell.\n",
    "    _, arg_params, aux_params = mx.rnn.load_rnn_checkpoint(stack, args.model_prefix, args.load_epoch)\n",
    "    model.set_params(arg_params, aux_params)\n",
    "\n",
    "    model.score(data_val, mx.metric.Perplexity(invalid_label),\n",
    "                batch_end_callback=mx.callback.Speedometer(args.batch_size, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "head = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=head)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "if args.num_layers >= 4 and len(args.gpus.split(',')) >= 4 and not args.stack_rnn:\n",
    "    print('WARNING: stack-rnn is recommended to train complex model on multiple GPUs')\n",
    "\n",
    "if args.test:\n",
    "    # Demonstrates how to load a model trained with CuDNN RNN and predict\n",
    "    # with non-fused MXNet symbol\n",
    "    test(args)\n",
    "else:\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BucketSeq2SeqIter(DataIter):\n",
    "    \"\"\"Simple bucketing iterator for sequence-to-sequence models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list of list of pairs of int\n",
    "        encoded sentences (source seq dict int, target seq dict int)\n",
    "    batch_size : int\n",
    "        batch_size of data    \n",
    "    invalid_label : int, default -1\n",
    "        key for invalid label, e.g. <end-of-sentence>\n",
    "    dtype : str, default 'float32'\n",
    "        data type\n",
    "    buckets : list of pairs of int (source seq length, target seq length)\n",
    "        size of data buckets. Automatically generated if None.\n",
    "    data_name : str, default 'data'\n",
    "        name of data    print(buckets)\n",
    "    label_name : str, default 'softmax_label'\n",
    "        name of label\n",
    "    layout : str\n",
    "        format of data and label. 'NT' means (batch_size, length)\n",
    "        and 'TN' means (length, batch_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_seq, output_seq, batch_size, buckets=None, invalid_label=-1,\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "                 layout='NTC'):\n",
    "        super(BucketSentenceIter, self).__init__()\n",
    "        if not buckets:\n",
    "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\n",
    "                       if j >= batch_size]\n",
    "        buckets.sort()\n",
    "\n",
    "        ndiscard = 0\n",
    "        self.data = [[] for _ in buckets]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            buck = bisect.bisect_left(buckets, len(sent))\n",
    "            if buck == len(buckets):\n",
    "                ndiscard += 1\n",
    "                continue\n",
    "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
    "            buff[:len(sent)] = sent\n",
    "            self.data[buck].append(buff)\n",
    "\n",
    "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\n",
    "\n",
    "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.invalid_label = invalid_label\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        self.major_axis = layout.find('N')\n",
    "        self.default_bucket_key = max(buckets)\n",
    "\n",
    "        if self.major_axis == 0:\n",
    "            self.provide_data = [(data_name, (batch_size, self.default_bucket_key))]\n",
    "            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]\n",
    "        elif self.major_axis == 1:\n",
    "            self.provide_data = [(data_name, (self.default_bucket_key, batch_size))]\n",
    "            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\n",
    "\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.data):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.idx)\n",
    "        for buck in self.data:\n",
    "            np.random.shuffle(buck)\n",
    "\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        for buck in self.data:\n",
    "            label = np.empty_like(buck)\n",
    "            label[:, :-1] = buck[:, 1:]\n",
    "            label[:, -1] = self.invalid_label\n",
    "            self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        if self.major_axis == 1:\n",
    "            data = self.nddata[i][j:j+self.batch_size].T\n",
    "            label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "        else:\n",
    "            data = self.nddata[i][j:j+self.batch_size]\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "        return DataBatch([data], [label],\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[(self.data_name, data.shape)],\n",
    "                         provide_label=[(self.label_name, label.shape)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
