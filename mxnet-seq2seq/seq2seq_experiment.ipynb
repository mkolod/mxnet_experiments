{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TODO\n",
    "\n",
    "- Return tuple dims with batch? (probably not necessary, but could do that) \n",
    "- Add <PAD> to the dictionary and reverse dictionary\n",
    "- Use deepdish to persist preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-30 00:02:00.444529. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from collections import namedtuple, Counter\n",
    "from unidecode import unidecode\n",
    "from itertools import groupby\n",
    "from mxnet.io import DataIter\n",
    "from random import shuffle\n",
    "\n",
    "import deepdish as dd\n",
    "\n",
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get rid of annoying Python deprecation warnings from built-in JSON encoder\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Args = namedtuple(\n",
    "    'Args', \n",
    "    ('test load_epoch num_layers num_hidden num_embed bidirectional gpus '\n",
    "     'kv_store num_epochs optimizer mom wd lr batch_size disp_batches '\n",
    "     'stack_rnn dropout model_prefix'))\n",
    "\n",
    "args = Args(\n",
    "    test=          False,\n",
    "    load_epoch=    0,\n",
    "    num_layers=    2,\n",
    "    num_hidden=    200,\n",
    "    num_embed=     200,\n",
    "    bidirectional= False,\n",
    "    gpus=          '0,1',\n",
    "    kv_store=      'device',\n",
    "    num_epochs=    1,\n",
    "    optimizer=    'adam',\n",
    "    mom=           0.9,\n",
    "    wd=           0.00001,\n",
    "    lr = 0.001,\n",
    "    batch_size=    32,\n",
    "    disp_batches=  50,\n",
    "    stack_rnn=     False,\n",
    "    dropout=       0.5,\n",
    "    model_prefix= 'foo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Decode text as UTF-8\n",
    "# Remove diacritical signs and convert to Latin alphabet\n",
    "# Separate punctuation as separate \"words\"\n",
    "def tokenize_text(fname, vocab=None, invalid_label=0, start_label=1, sep_punctuation=True):\n",
    "    lines = unidecode(open(fname).read().decode('utf-8')).split('\\n')\n",
    "    lines = [x for x in lines if x]\n",
    "    lines = map(lambda x: re.findall(r\"\\w+|[^\\w\\s]\", x, re.UNICODE), lines)    \n",
    "    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label, start_label=start_label)\n",
    "    return sentences, vocab\n",
    "\n",
    "Dataset = namedtuple('Dataset', ['src_sent', 'src_vocab', 'targ_sent', 'targ_vocab'])\n",
    "\n",
    "def get_data(src_path, targ_path, start_label=1, invalid_label=0):\n",
    "    src_sent, src_vocab = tokenize_text(src_path, start_label=start_label,\n",
    "                                invalid_label=invalid_label)\n",
    "    \n",
    "    targ_sent, targ_vocab = tokenize_text(targ_path, start_label=start_label, #new_start+1,\n",
    "                                          invalid_label=invalid_label)\n",
    "\n",
    "    return Dataset(src_sent=src_sent, src_vocab=src_vocab, targ_sent=targ_sent, targ_vocab=targ_vocab)\n",
    "\n",
    "def persist_dataset(dataset, path):\n",
    "    with open(path, 'wb+') as fileobj:\n",
    "        pickle.dump(dataset, fileobj)\n",
    "        \n",
    "def load_dataset(path):\n",
    "    with open(path, 'rb') as fileobj:\n",
    "        return pickle.load(fileobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = \\\n",
    "    get_data(\n",
    "        src_path='./data/europarl-v7.es-en.en_vsmall',\n",
    "        targ_path='./data/europarl-v7.es-en.es_vsmall',\n",
    "        start_label=1,\n",
    "        invalid_label=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = './foo.pickle'\n",
    "persist_dataset(dataset, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del dataset\n",
    "dataset = load_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gen_buckets(dataset, filter_smaller_counts_than=None, max_sent_len=60, min_sent_len=1):\n",
    "    src_sent = dataset.src_sent\n",
    "    targ_sent = dataset.targ_sent\n",
    "    length_pairs = map(lambda x: (len(x[0]), len(x[1])), zip(src_sent, targ_sent))\n",
    "    counts = list(Counter(length_pairs).items())\n",
    "    c_sorted = sorted(counts, key=operator.itemgetter(0, 1))\n",
    "    buckets = [i for i in c_sorted if i[1] >= filter_smaller_counts_than and \n",
    "               (max_sent_len is None or i[0][0] <= max_sent_len) and\n",
    "               (max_sent_len is None or i[0][1] <= max_sent_len) and\n",
    "               (min_sent_len is None or i[0][0] >= min_sent_len) and\n",
    "               (min_sent_len is None or i[0][1] >= min_sent_len)]\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_len = 60\n",
    "\n",
    "all_buckets = gen_buckets(dataset, filter_smaller_counts_than=None, max_sent_len=None, min_sent_len=None)\n",
    "batch_size = 32\n",
    "batched_buckets = gen_buckets(dataset, filter_smaller_counts_than=batch_size, max_sent_len=max_len, min_sent_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batched_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_sent = lambda sent: reduce(lambda a, b: a + b, map(lambda x: x[1], sent))\n",
    "num_all = num_sent(all_buckets)\n",
    "num_batched = num_sent(batched_buckets)\n",
    "print(\"# of all buckets: %d (# sent: %d)\" % (len(all_buckets), num_all))\n",
    "print(\"# of buckets with counts < %d filtered out: %d (num sent: %d)\" % (batch_size, len(batched_buckets), num_batched))\n",
    "print(\"percent of examples remaining after filtering: %.2f\" % (100.0*num_batched/num_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_sorted_buckets(buckets):\n",
    "    b = sorted(buckets, key=operator.itemgetter(0, 1))\n",
    "    for i in b:\n",
    "        print(i)\n",
    "\n",
    "print_sorted_buckets(batched_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src_sent = dataset.src_sent\n",
    "targ_sent = dataset.targ_sent\n",
    "\n",
    "sent_len = lambda x: map(lambda y: len(y), x)\n",
    "max_len = lambda x: max(sent_len(x))\n",
    "min_len = lambda x: min(sent_len(x))\n",
    "\n",
    "min_len = min(min(sent_len(src_sent)), min(sent_len(targ_sent)))import numpy as np\n",
    "import mxnet as mx\n",
    "from collections import namedtuple, Counter\n",
    "from unidecode import unidecode\n",
    "from itertools import groupby\n",
    "from mxnet.io import DataIter\n",
    "from random import shuffle\n",
    "\n",
    "import deepdish as dd\n",
    "\n",
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "# max_len = max(max(sent_len(src_sent)), max(sent_len(targ_sent)))\n",
    "\n",
    "# min_len = min\n",
    "max_len = 65\n",
    "increment = 5\n",
    "\n",
    "all_pairs = [(i, j) for i in range(\n",
    "        min_len,max_len+increment,increment\n",
    "    ) for j in range(\n",
    "        min_len,max_len+increment,increment\n",
    "    )]\n",
    "\n",
    "print(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TwoDBisect:\n",
    "    def __init__(self, buckets):\n",
    "        self.buckets = sorted(buckets, key=operator.itemgetter(0, 1))\n",
    "        self.x, self.y = zip(*buckets)\n",
    "        self.x, self.y = np.array(list(self.x)), np.array(list(self.y))\n",
    "        \n",
    "    def twod_bisect(self, source, target):    \n",
    "        offset1 = np.searchsorted(self.x, len(source), side='left')\n",
    "        offset2 = np.where(self.y[offset1:] >= len(target))[0]        \n",
    "        return self.buckets[offset1 + offset2[0]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bisect = TwoDBisect(all_pairs)\n",
    "bisect.buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tuples = []\n",
    "\n",
    "src_sent = dataset.src_sent\n",
    "targ_sent = dataset.targ_sent\n",
    "\n",
    "short_sentences = filter(lambda x: len(x[0]) <= max_len and len(x[1]) <= max_len, zip(src_sent, targ_sent))\n",
    "# def print_sorted_buckets(buckets):\n",
    "#     b = sorted(buckets, key=operator.itemgetter(0, 1))\n",
    "#     for i in b:\n",
    "#         print(i)\n",
    "\n",
    "# print_sorted_buckets(batched_buckets)\n",
    "for src, targ in short_sentences:\n",
    "    try:\n",
    "        len_tup = bisect.twod_bisect(src, targ)\n",
    "        rev_src = src[::-1] \n",
    "        tuples.append((src, targ, len_tup))\n",
    "    except Exception as e:\n",
    "        print(\"src length: %d\" % len(src))\n",
    "        print(\"targ length: %d\" % len(targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted_tuples = sorted(tuples, key=operator.itemgetter(2))\n",
    "sorted_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g = groupby(sorted_tuples, lambda x: x[2])\n",
    "groups = {}\n",
    "\n",
    "for i in g:\n",
    "#     lst = list(i[1])\n",
    "    count = 1 #len(lst)\n",
    "#     if count < batch_size:\n",
    "#         continue\n",
    "    groups[i[0]] = map(lambda x: list(x[:2]), i[1]) # lst\n",
    "    print(\"tuple: %s, count: %d\" % (str(i[0]), count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "groups[(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "invalid_symbol = -1\n",
    "\n",
    "groups2 = {}\n",
    "\n",
    "for tup, grouper in groups.items():\n",
    "    new_grouper = []\n",
    "    new_src = np.full((len(grouper), tup[0]), invalid_symbol, dtype=np.int32)\n",
    "    new_targ = np.full((len(grouper), tup[1]), invalid_symbol, dtype=np.int32)\n",
    "    \n",
    "    for idx, grp in enumerate(grouper):\n",
    "        source = grp[0]\n",
    "        target = grp[1]\n",
    "        rev_src = source[::-1]\n",
    "        new_src[idx, :-(len(rev_src)+1):-1] = rev_src\n",
    "        new_targ[idx, :len(target)] = target\n",
    "    new_grouper = (new_src, new_targ)\n",
    "    groups2[tup] = new_grouper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def iterate_groups(groups, batch_size=32):\n",
    "    \n",
    "    def chunks(l, n, trim_incomplete_batches=True):\n",
    "        n = max(1, n)\n",
    "        end = len(l)/n*n if trim_incomplete_batches else len(l)\n",
    "        return (l[i:i+n] for i in xrange(0, end, n))\n",
    "    \n",
    "    for key, group in groups.items():\n",
    "        num_examples = len(group[0])\n",
    "        indices = list(xrange(num_examples))\n",
    "        shuffle(indices)\n",
    "        src_sent, targ_sent = group\n",
    "        src_sent = src_sent[indices]\n",
    "        targ_sent = targ_sent[indices]        \n",
    "        for chunk in chunks(list(xrange(num_examples)), batch_size):\n",
    "            yield [src_sent[chunk], targ_sent[chunk]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gg = iterate_groups(groups2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        g = gg.next()\n",
    "        cnt += len(g[0])\n",
    "    except StopIteration:\n",
    "        print(\"End of iteration!\")\n",
    "        break\n",
    "        \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_dataset = {\n",
    "    'groups': groups,\n",
    "    'src_vocab': \n",
    "    'targ_vocab'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dd.io.save('groups.h5', groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "groups = dd.io.load('groups.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add dictionary and inverse dictionary to the serialized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "groups[(11,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print some inversed source sentences and target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # ctr = 0\n",
    "# # num_ex = 10\n",
    "# invalid_symbol = -9999\n",
    "\n",
    "# for tup, grouper in groups.items():\n",
    "# #     print(\"Tuple: %s\" % str(tup))\n",
    "#     new_grouper = []\n",
    "#     new_src = np.full((len(grouper), tup[0], 2), invalid_symbol)\n",
    "#     new_targ = np.full((len(grouper), tup[1], 2), invalid_symbol)\n",
    "    \n",
    "#     for grp in grouper:\n",
    "#         source, target = grp\n",
    "#         rev_src = source[::-1]\n",
    "#         if len(rev_src) < tup[0]:\n",
    "#             rev_src_2 = [invalid_symbol] * tup[0]\n",
    "#             rev_src_2[:-(len(rev_src)+1):-1] = rev_src\n",
    "#         else:\n",
    "#             rev_src_2 = rev_src\n",
    "#         if len(target) < tup[1]:\n",
    "#             targ_2 = [invalid_symbol] * tup[1]\n",
    "#             targ_2[:len(target)] = target\n",
    "#         else:\n",
    "#             targ_2 = target\n",
    "#         new_grouper.append([rev_src_2, targ_2])\n",
    "#         groups[tup] = new_grouper #np.array(new_grouper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grp = groups[(11,11)]\n",
    "print(\"shape: %s\" % str(np.shape(grp)))\n",
    "np.shape(grp[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "num_cases = 2\n",
    "\n",
    "for tup, grouper in groups.items():\n",
    "    print(\"tuple: %s\" % str(tup))\n",
    "    print(grouper)\n",
    "    print(\"\")\n",
    "    ctr += 1\n",
    "    if ctr >= num_cases:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counts = list(Counter(tuples).items())\n",
    "batch_size = 32\n",
    "filter_smaller_counts_than = batch_size\n",
    "\n",
    "c_sorted = sorted(counts, key=operator.itemgetter(0, 1))\n",
    "buckets = [i for i in c_sorted if i[1] >= filter_smaller_counts_than]\n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BucketSeq2SeqIter(DataIter):\n",
    "    \"\"\"Simple bucketing iterator for sequence-to-sequence models.\n",
    "    \n",
    "    @staticmethod\n",
    "    def \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    source_seq : list of list of int\n",
    "    target_seq : list of list of int\n",
    "    batch_size : int\n",
    "        batch_size of data    \n",
    "    invalid_label : int, default -1\n",
    "        key for invalid label, e.g. <end-of-sentence>\n",
    "    dtype : str, default 'float32'\n",
    "        data type\n",
    "    buckets : list of pairs of int (source seq length, target seq length)\n",
    "        size of data buckets. Automatically generated if None.\n",
    "    data_name : str, default 'data'\n",
    "        name of data    print(buckets)\n",
    "    label_name : str, default 'softmax_label'\n",
    "        name of label\n",
    "    layout : str\n",
    "        format of data and label. 'NT' means (batch_size, length)\n",
    "        and 'TN' means (length, batch_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, source, target, batch_size, buckets=None, invalid_label=-1,\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "                 layout='NTC', min_sent_len = 1, max_sent_len = 60):\n",
    "        super(BucketSeq2SeqIter, self).__init__()\n",
    "        \n",
    "        # insert call to gen_buckets here if buckets are not provided\n",
    "        if not buckets:\n",
    "            all_pairs = [(i, j) for i in range(\n",
    "                min_len,max_len+increment,increment\n",
    "            ) for j in range(\n",
    "                min_len,max_len+increment,increment\n",
    "            )]\n",
    "            self.bisect = TwoDBisect(buckets = all_pairs)\n",
    "            buckets = \n",
    "            \n",
    "#             [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\n",
    "#                        if j >= batch_size]\n",
    "            \n",
    "        # Sorting is kinda pointless because it's first by the first element of the tuple,\n",
    "        # then the next. So, it could be [(1, 2), (1, 20), (2, 5), (2, 71)]\n",
    "#         buckets.sort()\n",
    "\n",
    "        ndiscard = 0\n",
    "        self.data = [[] for _ in buckets]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            \n",
    "            # this bisect also won't work because it's now based on a tuple of lengths\n",
    "            buck = bisect.bisect_left(buckets, len(sent))\n",
    "            # this test is not appropriate because of the tuple of lengths\n",
    "            if buck == len(buckets):\n",
    "                ndiscard += 1\n",
    "                continue\n",
    "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
    "            buff[:len(sent)] = sent\n",
    "            self.data[buck].append(buff)\n",
    "\n",
    "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\n",
    "\n",
    "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.invalid_label = invalid_label\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        self.major_axis = layout.find('N')\n",
    "        self.default_bucket_key = max(buckets)\n",
    "\n",
    "        if self.major_axis == 0:\n",
    "            self.provide_data = [(data_name, (batch_size, self.default_bucket_key))]\n",
    "            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]\n",
    "        elif self.major_axis == 1:\n",
    "            self.provide_data = [(data_name, (self.default_bucket_key, batch_size))]\n",
    "            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\n",
    "\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.data):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.idx)\n",
    "        for buck in self.data:\n",
    "            np.random.shuffle(buck)\n",
    "\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        for buck in self.data:\n",
    "            label = np.empty_like(buck)\n",
    "            label[:, :-1] = buck[:, 1:]\n",
    "            label[:, -1] = self.invalid_label\n",
    "            self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        if self.major_axis == 1:\n",
    "            data = self.nddata[i][j:j+self.batch_size].T\n",
    "            label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "        else:\n",
    "            data = self.nddata[i][j:j+self.batch_size]\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "        return DataBatch([data], [label],\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[(self.data_name, data.shape)],\n",
    "                         provide_label=[(self.label_name, label.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(batched_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "buckets = list(range(5, 60, 5))\n",
    "#[10, 20, 30, 40, 50, 60]\n",
    "\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "start_label = 1\n",
    "invalid_label = 0\n",
    "\n",
    "def tokenize_text(fname, vocab=None, invalid_label=-1, start_label=0):\n",
    "    lines = open(fname).readlines()\n",
    "    lines = [filter(None, i.split(' ')) for i in lines]\n",
    "    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label, start_label=start_label)\n",
    "    return sentences, vocab\n",
    "\n",
    "def get_data(layout):\n",
    "    train_sent, vocab = tokenize_text(\"./data/ptb.train.txt\", start_label=start_label,\n",
    "                                      invalid_label=invalid_label)\n",
    "    val_sent, _ = tokenize_text(\"./data/ptb.test.txt\", vocab=vocab, start_label=start_label,\n",
    "                                invalid_label=invalid_label)\n",
    "\n",
    "    data_train  = mx.rnn.BucketSentenceIter(train_sent, args.batch_size, buckets=buckets,\n",
    "                                            invalid_label=invalid_label, layout=layout)\n",
    "    data_val    = mx.rnn.BucketSentenceIter(val_sent, args.batch_size, buckets=buckets,\n",
    "                                            invalid_label=invalid_label, layout=layout)\n",
    "    \n",
    "    print(\"default: %s\" % data_train.default_bucket_key)\n",
    "    return data_train, data_val, vocab\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    data_train, data_val, vocab = get_data('TN')\n",
    "    if args.stack_rnn:\n",
    "        stack = mx.rnn.SequentialRNNCell()\n",
    "        for layer in range(args.num_layers):\n",
    "            dropout = 0.0\n",
    "            if layer < (args.num_layers - 1):\n",
    "                dropout = args.dropout\n",
    "            stack.add(mx.rnn.FusedRNNCell(args.num_hidden, num_layers=1,\n",
    "                    mode='lstm', prefix='lstm_%d'%layer, dropout=dropout,\n",
    "                    bidirectional=args.bidirectional))\n",
    "        cell = stack\n",
    "    else:\n",
    "        cell = mx.rnn.FusedRNNCell(args.num_hidden, num_layers=args.num_layers, dropout=args.dropout,\n",
    "                mode='lstm', bidirectional=args.bidirectional)\n",
    "\n",
    "    def sym_gen(seq_len):\n",
    "        data = mx.sym.Variable('data')\n",
    "        label = mx.sym.Variable('softmax_label')\n",
    "        embed = mx.sym.Embedding(data=data, input_dim=len(vocab), output_dim=args.num_embed,name='embed')\n",
    "\n",
    "        output, _ = cell.unroll(seq_len, inputs=embed, merge_outputs=True, layout='TNC')\n",
    "\n",
    "        pred = mx.sym.Reshape(output,\n",
    "                shape=(-1, args.num_hidden*(1+args.bidirectional)))\n",
    "        pred = mx.sym.FullyConnected(data=pred, num_hidden=len(vocab), name='pred')\n",
    "\n",
    "        label = mx.sym.Reshape(label, shape=(-1,))\n",
    "        pred = mx.sym.SoftmaxOutput(data=pred, label=label, name='softmax')\n",
    "\n",
    "        return pred, ('data',), ('softmax_label',)\n",
    "\n",
    "    if args.gpus:\n",
    "        contexts = [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "    else:\n",
    "        contexts = mx.cpu(0)\n",
    "\n",
    "    model = mx.mod.BucketingModule(\n",
    "        sym_gen             = sym_gen,\n",
    "        default_bucket_key  = data_train.default_bucket_key,\n",
    "        context             = contexts)\n",
    "\n",
    "    if args.load_epoch:\n",
    "        _, arg_params, aux_params = mx.rnn.load_rnn_checkpoint(\n",
    "            cell, args.model_prefix, args.load_epoch)\n",
    "    else:\n",
    "        arg_params = None\n",
    "        aux_params = None\n",
    "\n",
    "    opt_params = {\n",
    "      'learning_rate': args.lr,\n",
    "      'wd': args.wd\n",
    "    }\n",
    "\n",
    "    if args.optimizer not in ['adadelta', 'adagrad', 'adam', 'rmsprop']:\n",
    "        opt_params['momentum'] = args.mom\n",
    "\n",
    "    model.fit(\n",
    "        train_data          = data_train,\n",
    "        eval_data           = data_val,\n",
    "        eval_metric         = mx.metric.Perplexity(invalid_label),\n",
    "        kvstore             = args.kv_store,\n",
    "        optimizer           = args.optimizer,\n",
    "        optimizer_params    = opt_params, \n",
    "        initializer         = mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n",
    "        arg_params          = arg_params,\n",
    "        aux_params          = aux_params,\n",
    "        begin_epoch         = args.load_epoch,\n",
    "        num_epoch           = args.num_epochs,\n",
    "        batch_end_callback  = mx.callback.Speedometer(args.batch_size, args.disp_batches),\n",
    "        epoch_end_callback  = mx.rnn.do_rnn_checkpoint(cell, args.model_prefix, 1)\n",
    "                              if args.model_prefix else None)\n",
    "\n",
    "def test(args):\n",
    "    assert args.model_prefix, \"Must specifiy path to load from\"\n",
    "    _, data_val, vocab = get_data('NT')\n",
    "\n",
    "    if not args.stack_rnn:\n",
    "        stack = mx.rnn.FusedRNNCell(args.num_hidden, num_layers=args.num_layers,\n",
    "                mode='lstm', bidirectional=args.bidirectionclass BucketSentenceIter(DataIter):\n",
    "    \"\"\"Simple bucketing iterator for language model.\n",
    "    Label for each step is constructed from data of\n",
    "    next step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list of list of int\n",
    "        encoded sentences\n",
    "    batch_size : int\n",
    "        batch_size of data\n",
    "    invalid_label : int, default -1\n",
    "        key for invalid label, e.g. <end-of-sentence>\n",
    "    dtype : str, default 'float32'\n",
    "        data type\n",
    "    buckets : list of int\n",
    "        size of data buckets. Automatically generated if None.\n",
    "    data_name : str, default 'data'\n",
    "        name of data\n",
    "    label_name : str, default 'softmax_label'\n",
    "        name of label\n",
    "    layout : str\n",
    "        format of data and label. 'NT' means (batch_size, length)\n",
    "        and 'TN' means (length, batch_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences, batch_size, buckets=None, invalid_label=-1,\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "                 layout='NTC'):\n",
    "        super(BucketSentenceIter, self).__init__()\n",
    "        if not buckets:\n",
    "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\n",
    "                       if j >= batch_size]\n",
    "        buckets.sort()\n",
    "\n",
    "        ndiscard = 0\n",
    "        self.data = [[] for _ in buckets]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            buck = bisect.bisect_left(buckets, len(sent))\n",
    "            if buck == len(buckets):\n",
    "                ndiscard += 1\n",
    "                continue\n",
    "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
    "            buff[:len(sent)] = sent\n",
    "            self.data[buck].append(buff)\n",
    "\n",
    "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\n",
    "\n",
    "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.invalid_label = invalid_label\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        self.major_axis = layout.find('N')\n",
    "        self.default_bucket_key = max(buckets)\n",
    "\n",
    "        if self.major_axis == 0:\n",
    "            self.provide_data = [(data_name, (batch_size, self.default_bucket_key))]\n",
    "            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]\n",
    "        elif self.major_axis == 1:\n",
    "            self.provide_data = [(data_name, (self.default_bucket_key, batch_size))]\n",
    "            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\n",
    "\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.data):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.idx)\n",
    "        for buck in self.data:\n",
    "            np.random.shuffle(buck)\n",
    "\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        for buck in self.data:\n",
    "            label = np.empty_like(buck)\n",
    "            label[:, :-1] = buck[:, 1:]\n",
    "            label[:, -1] = self.invalid_label\n",
    "            self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        if self.major_axis == 1:\n",
    "            data = self.nddata[i][j:j+self.batch_size].T\n",
    "            label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "        else:\n",
    "            data = self.nddata[i][j:j+self.batch_size]\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "        return DataBatch([data], [label],\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[(self.data_name, data.shape)],\n",
    "                         provide_label=[(self.label_name, label.shape)])al).unfuse()\n",
    "    else:\n",
    "        stack = mx.rnn.SequentialRNNCell()\n",
    "        for i in range(args.num_layers):\n",
    "            cell = mx.rnn.LSTMCell(num_hidden=args.num_hidden, prefix='lstm_%dl0_'%i)\n",
    "            if args.bidirectional:\n",
    "                cell = mx.rnn.BidirectionalCell(\n",
    "                        cell,\n",
    "                        mx.rnn.LSTMCell(num_hidden=args.num_hidden, prefix='lstm_%dr0_'%i),\n",
    "                        output_prefix='bi_lstm_%d'%i)\n",
    "            stack.add(cell)\n",
    "\n",
    "    def sym_gen(seq_len):\n",
    "        data = mx.sym.Variable('data')\n",
    "        label = mx.sym.Variable('softmax_label')\n",
    "        embed = mx.sym.Embedding(data=data, input_dim=len(vocab),\n",
    "                                 output_dim=args.num_embed, name='embed')\n",
    "\n",
    "        stack.reset()\n",
    "        outputs, states = stack.unroll(seq_len, inputs=embed, merge_outputs=True)\n",
    "\n",
    "        pred = mx.sym.Reshape(outputs,\n",
    "                shape=(-1, args.num_hidden*(1+args.bidirectional)))\n",
    "        pred = mx.sym.FullyConnected(data=pred, num_hidden=len(vocab), name='pred')\n",
    "\n",
    "        label = mx.sym.Reshape(label, shape=(-1,))\n",
    "        pred = mx.sym.SoftmaxOutput(data=pred, label=label, name='softmax')\n",
    "\n",
    "        return pred, ('data',), ('softmax_label',)\n",
    "    \n",
    "class BucketSentenceIter(DataIter):\n",
    "    \"\"\"Simple bucketing iterator for language model.\n",
    "    Label for each step is constructed from data of\n",
    "    next step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list of list of int\n",
    "        encoded sentences\n",
    "    batch_size : int\n",
    "        batch_size of data\n",
    "    invalid_label : int, default -1\n",
    "        key for invalid label, e.g. <end-of-sentence>\n",
    "    dtype : str, default 'float32'\n",
    "        data type\n",
    "    buckets : list of int\n",
    "        size of data buckets. Automatically generated if None.\n",
    "    data_name : str, default 'data'\n",
    "        name of data\n",
    "    label_name : str, default 'softmax_label'\n",
    "        name of label\n",
    "    layout : str\n",
    "        format of data and label. 'NT' means (batch_size, length)\n",
    "        and 'TN' means (length, batch_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences, batch_size, buckets=None, invalid_label=-1,\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "                 layout='NTC'):\n",
    "        super(BucketSentenceIter, self).__init__()\n",
    "        if not buckets:\n",
    "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\n",
    "                       if j >= batch_size]\n",
    "        buckets.sort()\n",
    "\n",
    "        ndiscard = 0\n",
    "        self.data = [[] for _ in buckets]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            buck = bisect.bisect_left(buckets, len(sent))\n",
    "            if buck == len(buckets):\n",
    "                ndiscard += 1\n",
    "                continue\n",
    "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
    "            buff[:len(sent)] = sent\n",
    "            self.data[buck].append(buff)\n",
    "\n",
    "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\n",
    "\n",
    "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.invalid_label = invalid_label\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        self.major_axis = layout.find('N')\n",
    "        self.default_bucket_key = max(buckets)\n",
    "\n",
    "        if self.major_axis == 0:\n",
    "            self.provide_data = [(data_name, (batch_size, self.default_bucket_key))]\n",
    "            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]\n",
    "        elif self.major_axis == 1:\n",
    "            self.provide_data = [(data_name, (self.default_bucket_key, batch_size))]\n",
    "            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\n",
    "\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.data):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.idx)\n",
    "        for buck in self.data:\n",
    "            np.random.shuffle(buck)\n",
    "\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        for buck in self.data:\n",
    "            label = np.empty_like(buck)\n",
    "            label[:, :-1] = buck[:, 1:]\n",
    "            label[:, -1] = self.invalid_label\n",
    "            self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        if self.major_axis == 1:\n",
    "            data = self.nddata[i][j:j+self.batch_size].T\n",
    "            label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "        else:\n",
    "            data = self.nddata[i][j:j+self.batch_size]\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "        return DataBatch([data], [label],\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[(self.data_name, data.shape)],\n",
    "                         provide_label=[(self.label_name, label.shape)])\n",
    "\n",
    "    if args.gpus:\n",
    "        contexts = [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "    else:\n",
    "        contexts = mx.cpu(0)\n",
    "\n",
    "    model = mx.mod.BucketingModule(\n",
    "        sym_gen             = sym_gen,\n",
    "        default_bucket_key  = data_val.default_bucket_key,\n",
    "        context             = contexts)\n",
    "    model.bind(data_val.provide_data, data_val.provide_label, for_training=False)\n",
    "\n",
    "    # note here we load using SequentialRNNCell instead of FusedRNNCell.\n",
    "    _, arg_params, aux_params = mx.rnn.load_rnn_checkpoint(stack, args.model_prefix, args.load_epoch)\n",
    "    model.set_params(arg_params, aux_params)\n",
    "\n",
    "    model.score(data_val, mx.metric.Perplexity(invalid_label),\n",
    "                batch_end_callback=mx.callback.Speedometer(args.batch_size, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "head = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=head)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "if args.num_layers >= 4 and len(args.gpus.split(',')) >= 4 and not args.stack_rnn:\n",
    "    print('WARNING: stack-rnn is recommended to train complex model on multiple GPUs')\n",
    "\n",
    "if args.test:\n",
    "    # Demonstrates how to load a model trained with CuDNN RNN and predict\n",
    "    # with non-fused MXNet symbol\n",
    "    test(args)\n",
    "else:\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BucketSeq2SeqIter(DataIter):\n",
    "    \"\"\"Simple bucketing iterator for sequence-to-sequence models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list of list of pairs of int\n",
    "        encoded sentences (source seq dict int, target seq dict int)\n",
    "    batch_size : int\n",
    "        batch_size of data    \n",
    "    invalid_label : int, default -1\n",
    "        key for invalid label, e.g. <end-of-sentence>\n",
    "    dtype : str, default 'float32'\n",
    "        data type\n",
    "    buckets : list of pairs of int (source seq length, target seq length)\n",
    "        size of data buckets. Automatically generated if None.\n",
    "    data_name : str, default 'data'\n",
    "        name of data    print(buckets)\n",
    "    label_name : str, default 'softmax_label'\n",
    "        name of label\n",
    "    layout : str\n",
    "        format of data and label. 'NT' means (batch_size, length)\n",
    "        and 'TN' means (length, batch_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_seq, output_seq, batch_size, buckets=None, invalid_label=-1,\n",
    "                 data_name='data', label_name='softmax_label', dtype='float32',\n",
    "                 layout='NTC'):\n",
    "        super(BucketSentenceIter, self).__init__()\n",
    "        if not buckets:\n",
    "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\n",
    "                       if j >= batch_size]\n",
    "        buckets.sort()\n",
    "\n",
    "        ndiscard = 0\n",
    "        self.data = [[] for _ in buckets]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            buck = bisect.bisect_left(buckets, len(sent))\n",
    "            if buck == len(buckets):\n",
    "                ndiscard += 1\n",
    "                continue\n",
    "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
    "            buff[:len(sent)] = sent\n",
    "            self.data[buck].append(buff)\n",
    "\n",
    "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\n",
    "\n",
    "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buckets = buckets\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.invalid_label = invalid_label\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        self.major_axis = layout.find('N')\n",
    "        self.default_bucket_key = max(buckets)\n",
    "\n",
    "        if self.major_axis == 0:\n",
    "            self.provide_data = [(data_name, (batch_size, self.default_bucket_key))]\n",
    "            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]\n",
    "        elif self.major_axis == 1:\n",
    "            self.provide_data = [(data_name, (self.default_bucket_key, batch_size))]\n",
    "            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\n",
    "\n",
    "        self.idx = []\n",
    "        for i, buck in enumerate(self.data):\n",
    "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\n",
    "        self.curr_idx = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.idx)\n",
    "        for buck in self.data:\n",
    "            np.random.shuffle(buck)\n",
    "\n",
    "        self.nddata = []\n",
    "        self.ndlabel = []\n",
    "        for buck in self.data:\n",
    "            label = np.empty_like(buck)\n",
    "            label[:, :-1] = buck[:, 1:]\n",
    "            label[:, -1] = self.invalid_label\n",
    "            self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "            self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "    def next(self):\n",
    "        if self.curr_idx == len(self.idx):\n",
    "            raise StopIteration\n",
    "        i, j = self.idx[self.curr_idx]\n",
    "        self.curr_idx += 1\n",
    "\n",
    "        if self.major_axis == 1:\n",
    "            data = self.nddata[i][j:j+self.batch_size].T\n",
    "            label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "        else:\n",
    "            data = self.nddata[i][j:j+self.batch_size]\n",
    "            label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "        return DataBatch([data], [label],\n",
    "                         bucket_key=self.buckets[i],\n",
    "                         provide_data=[(self.data_name, data.shape)],\n",
    "                         provide_label=[(self.label_name, label.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AttentionEncoderCell(BaseRNNCell):\n",
    "   \"\"\"Place holder cell that prepare input for attention decoders\"\"\"\n",
    "   def __init__(self, prefix='encode_', params=None):\n",
    "       super(AttentionEncoderCell, self).__init__(prefix, params=params)\n",
    "\n",
    "   @property\n",
    "   def state_shape(self):\n",
    "       return []\n",
    "\n",
    "   def __call__(self, inputs, states):\n",
    "       return inputs, states + [symbol.expand_dims(inputs, axis=1)]\n",
    "\n",
    "   def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):\n",
    "       outputs = _normalize_sequence(length, inputs, layout, merge_outputs)\n",
    "       if merge_outputs is True:\n",
    "           states = outputs\n",
    "       else:\n",
    "           states = inputs\n",
    "\n",
    "       # attention cell always use NTC layout for states\n",
    "       states, _ = _normalize_sequence(None, states, 'NTC', True, layout)\n",
    "       return outputs, [states]\n",
    "\n",
    "\n",
    "def _attention_pooling(source, scores):\n",
    "   # source: (batch_size, seq_len, encoder_num_hidden)\n",
    "   # scores: (batch_size, seq_len, 1)\n",
    "   probs = symbol.softmax(scores, axis=1)\n",
    "   output = symbol.batch_dot(source, probs, transpose_a=True)\n",
    "   return symbol.reshape(output, shape=(0, 0))\n",
    "\n",
    "\n",
    "class BaseAttentionCell(BaseRNNCell):\n",
    "   \"\"\"Base class for attention cells\"\"\"\n",
    "   def __init__(self, prefix='att_', params=None):\n",
    "       super(BaseAttentionCell, self).__init__(prefix, params=params)\n",
    "\n",
    "   @property\n",
    "   def state_shape(self):\n",
    "       return [(0, 0, 0)]\n",
    "\n",
    "   def __call__(self, inputs, states):\n",
    "       raise NotImplementedError\n",
    "\n",
    "\n",
    "class DotAttentionCell(BaseAttentionCell):\n",
    "   \"\"\"Dot attention\"\"\"\n",
    "   def __init__(self, prefix='dotatt_', params=None):\n",
    "       super(DotAttentionCell, self).__init__(prefix, params=params)\n",
    "\n",
    "   def __call__(self, inputs, states):\n",
    "       # inputs: (batch_size, decoder_num_hidden)\n",
    "       # for dot attention decoder_num_hidden must equal encoder_num_hidden\n",
    "       if len(states) > 1:\n",
    "           states = [symbol.concat(*states, dim=1)]\n",
    "\n",
    "       # source: (batch_size, seq_len, encoder_num_hidden)\n",
    "       source = states[0]\n",
    "       # (batch_size, decoder_num_hidden, 1)\n",
    "       inputs = symbol.expand_dims(inputs, axis=2)\n",
    "       # (batch_size, seq_len, 1)\n",
    "       scores = symbol.batch_dot(source, inputs)\n",
    "       # (batch_size, encoder_num_hidden)\n",
    "       return _attention_pooling(source, scores), states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AttentionEncoderCell(BaseRNNCell):\n",
    "   \"\"\"Place holder cell that prepare input for attention decoders\"\"\"\n",
    "   def __init__(self, prefix='encode_', params=None):\n",
    "       super(AttentionEncoderCell, self).__init__(prefix, params=params)\n",
    "\n",
    "   @property\n",
    "   def state_shape(self):\n",
    "       return []\n",
    "\n",
    "   def __call__(self, inputs, states):\n",
    "       return inputs, states + [symbol.expand_dims(inputs, axis=1)]\n",
    "\n",
    "   def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):\n",
    "       outputs = _normalize_sequence(length, inputs, layout, merge_outputs)\n",
    "       if merge_outputs is True:\n",
    "           states = outputs\n",
    "       else:\n",
    "           states = inputs\n",
    "\n",
    "       # attention cell always use NTC layout for states\n",
    "       states, _ = _normalize_sequence(None, states, 'NTC', True, layout)\n",
    "       return outputs, [states]\n",
    "\n",
    "\n",
    "def _attention_pooling(source, scores):\n",
    "   # source: (batch_size, seq_len, encoder_num_hidden)\n",
    "   # scores: (batch_size, seq_len, 1)\n",
    "   probs = symbol.softmax(scores, axis=1)\n",
    "   output = symbol.batch_dot(source, probs, transpose_a=True)\n",
    "   return symbol.reshape(output, shape=(0, 0))\n",
    "\n",
    "\n",
    "class BaseAttentionCell(BaseRNNCell):\n",
    "   \"\"\"Base class for attention cells\"\"\"\n",
    "   def __init__(self, prefix='att_', params=None):\n",
    "       super(BaseAttentionCell, self).__init__(prefix, params=params)\n",
    "\n",
    "   @property\n",
    "   def state_shape(self):\n",
    "       return [(0, 0, 0)]\n",
    "\n",
    "   def __call__(self, inputs, states):\n",
    "       raise NotImplementedError\n",
    "\n",
    "\n",
    "class DotAttentionCell(BaseAttentionCell):\n",
    "   \"\"\"Dot attention\"\"\"\n",
    "   def __init__(self, prefix='dotatt_', params=None):\n",
    "       super(DotAttentionCell, self).__init__(prefix, params=params)\n",
    "\n",
    "   def __call__(self, inputs, states):\n",
    "       # inputs: (batch_size, decoder_num_hidden)\n",
    "       # for dot attention decoder_num_hidden must equal encoder_num_hidden\n",
    "       if len(states) > 1:\n",
    "           states = [symbol.concat(*states, dim=1)]\n",
    "\n",
    "       # source: (batch_size, seq_len, encoder_num_hidden)\n",
    "       source = states[0]\n",
    "       # (batch_size, decoder_num_hidden, 1)\n",
    "       inputs = symbol.expand_dims(inputs, axis=2)\n",
    "       # (batch_size, seq_len, 1)\n",
    "       scores = symbol.batch_dot(source, inputs)\n",
    "       # (batch_size, encoder_num_hidden)\n",
    "       return _attention_pooling(source, scores), states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
