{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-31 16:24:04.598330. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from collections import namedtuple, Counter\n",
    "from unidecode import unidecode\n",
    "from itertools import groupby\n",
    "from mxnet.io import DataIter\n",
    "from random import shuffle\n",
    "\n",
    "import deepdish as dd\n",
    "\n",
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get rid of annoying Python deprecation warnings from built-in JSON encoder\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Decode text as UTF-8\n",
    "# Remove diacritical signs and convert to Latin alphabet\n",
    "# Separate punctuation as separate \"words\"\n",
    "def tokenize_text(fname, vocab=None, invalid_label=0, start_label=1, sep_punctuation=True):\n",
    "    lines = unidecode(open(fname).read().decode('utf-8')).split('\\n')\n",
    "    lines = [x for x in lines if x]\n",
    "    lines = map(lambda x: re.findall(r\"\\w+|[^\\w\\s]\", x, re.UNICODE), lines)    \n",
    "    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label, start_label=start_label)\n",
    "    return sentences, vocab\n",
    "\n",
    "Dataset = namedtuple(\n",
    "    'Dataset', \n",
    "    ['src_sent', 'src_vocab', 'inv_src_vocab', 'targ_sent', 'targ_vocab', 'inv_targ_vocab'])\n",
    "\n",
    "def invert_dict(d):\n",
    "    return {v: k for k, v in d.iteritems()}\n",
    "\n",
    "\n",
    "def get_data(src_path, targ_path, start_label=1, invalid_label=0, pad_symbol='<PAD>'):\n",
    "    src_sent, src_vocab = tokenize_text(src_path, start_label=start_label,\n",
    "                                invalid_label=invalid_label)\n",
    "    \n",
    "    src_vocab[pad_symbol] = invalid_label\n",
    "    inv_src_vocab = invert_dict(src_vocab)\n",
    "\n",
    "    targ_sent, targ_vocab = tokenize_text(targ_path, start_label=start_label, #new_start+1,\n",
    "                                          invalid_label=invalid_label)\n",
    "    \n",
    "    targ_vocab[pad_symbol] = invalid_label\n",
    "    inv_targ_vocab = invert_dict(targ_vocab)\n",
    "    \n",
    "    return Dataset(\n",
    "        src_sent=src_sent, src_vocab=src_vocab, inv_src_vocab=inv_src_vocab,\n",
    "        targ_sent=targ_sent, targ_vocab=targ_vocab, inv_targ_vocab=inv_targ_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def persist_dataset(dataset, path):\n",
    "    with open(path, 'wb+') as fileobj:\n",
    "        pickle.dump(dataset, fileobj)\n",
    "        \n",
    "def load_dataset(path):\n",
    "    with open(path, 'rb') as fileobj:\n",
    "        return pickle.load(fileobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = \\\n",
    "    get_data(\n",
    "        src_path='./data/europarl-v7.es-en.en_small',\n",
    "        targ_path='./data/europarl-v7.es-en.es_small',\n",
    "        start_label=1,\n",
    "        invalid_label=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqIterator:    \n",
    "\n",
    "    class TwoDBisect:\n",
    "        def __init__(self, buckets):\n",
    "            self.buckets = sorted(buckets, key=operator.itemgetter(0, 1))\n",
    "            self.x, self.y = zip(*buckets)\n",
    "            self.x, self.y = np.array(list(self.x)), np.array(list(self.y))\n",
    "\n",
    "        def twod_bisect(self, source, target):    \n",
    "            offset1 = np.searchsorted(self.x, len(source), side='left')\n",
    "            offset2 = np.where(self.y[offset1:] >= len(target))[0]        \n",
    "            return self.buckets[offset1 + offset2[0]]     \n",
    "    \n",
    "    def __init__(self, dataset, buckets=None, batch_size=32, max_sent_len=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.src_sent = dataset.src_sent\n",
    "        self.targ_sent = dataset.targ_sent\n",
    "        if buckets:\n",
    "            z = zip(*buckets)\n",
    "            self.max_sent_len = max(max(z[0]), max(z[1]))\n",
    "        else:\n",
    "            self.max_sent_len = max_sent_len\n",
    "        if self.max_sent_len:\n",
    "            self.src_sent, self.targ_sent = self.filter_long_sent(\n",
    "                self.src_sent, self.targ_sent, self.max_sent_len) \n",
    "        self.src_vocab = dataset.src_vocab\n",
    "        self.targ_vocab = dataset.targ_vocab\n",
    "        self.inv_src_vocab = dataset.inv_src_vocab\n",
    "        self.inv_targ_vocab = dataset.inv_targ_vocab\n",
    "        # Can't filter smaller counts per bucket if those sentences still exist!\n",
    "        self.buckets = buckets if buckets else self.gen_buckets(\n",
    "            self.src_sent, self.targ_sent, filter_smaller_counts_than=1, max_sent_len=max_sent_len)\n",
    "        self.bisect = Seq2SeqIterator.TwoDBisect(self.buckets)\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.pad_id = self.src_vocab['<PAD>']\n",
    "        # After bucketization, we should probably del self.src_sent and self.targ_sent\n",
    "        # to free up memory.\n",
    "        self.bucketed_data, self.bucket_idx_to_key = self.bucketize()\n",
    "        self.bucket_key_to_idx = invert_dict(dict(enumerate(self.bucket_idx_to_key)))\n",
    "        self.interbucket_idx = 0\n",
    "        self.num_buckets = len(self.bucket_idx_to_key)\n",
    "        self.bucket_iterator_indices = list(range(self.num_buckets))\n",
    "#         self.reset()\n",
    "\n",
    "    \n",
    "    def bucketize(self):\n",
    "        tuples = []\n",
    "        ctr = 0\n",
    "        for src, targ in zip(self.src_sent, self.targ_sent):\n",
    "            len_tup = self.bisect.twod_bisect(src, targ)\n",
    "            rev_src = src[::-1] \n",
    "            tuples.append((src, targ, len_tup))\n",
    "            \n",
    "        sorted_tuples = sorted(tuples, key=operator.itemgetter(2))\n",
    "        grouped = groupby(sorted_tuples, lambda x: x[2])\n",
    "        bucketed_data = [] \n",
    "        bucket_idx_to_key = []\n",
    "        \n",
    "        for group in grouped:\n",
    "            \n",
    "            # get src and targ sentences, ignore the last elem of the tuple \n",
    "            # (the grouping key of (src_len, targ_len))\n",
    "            key, value = group[0], map(lambda x: x[:2], group[1])\n",
    "\n",
    "            # create padded representation\n",
    "            new_src = np.full((len(value), key[0]), self.pad_id, dtype=np.int32)\n",
    "            new_targ = np.full((len(value), key[1]), self.pad_id, dtype=np.int32)\n",
    "            \n",
    "            for idx, example in enumerate(value):\n",
    "                curr_src, curr_targ = example\n",
    "                rev_src = curr_src[::-1]\n",
    "                new_src[idx, :-(len(rev_src)+1):-1] = rev_src\n",
    "                new_targ[idx, :len(curr_targ)] = curr_targ\n",
    "                                \n",
    "            bucketed_data.append((new_src, new_targ))\n",
    "            bucket_idx_to_key.append(key)\n",
    "        return bucketed_data, bucket_idx_to_key\n",
    "    \n",
    "    def current_bucket_key(self):\n",
    "        return self.bucket_idx_to_key[self.interbucket_idx]\n",
    "    \n",
    "    def current_bucket_index(self):\n",
    "        return self.bucket_iterator_indices[self.interbucket_idx]\n",
    "\n",
    "    # shuffle the data within buckets, and reset iterator\n",
    "    def reset(self):\n",
    "        self.interbucket_idx = 0\n",
    "        for idx in xrange(len(self.bucketed_data)):\n",
    "            current = self.bucketed_data[idx]\n",
    "            src, targ = current\n",
    "            indices = np.array(range(len(src)))\n",
    "            np.random.shuffle(indices)\n",
    "            src = src[indices]\n",
    "            targ = targ[indices]\n",
    "            self.bucketed_data[idx] = (src, targ)\n",
    "        shuffle(self.bucket_iterator_indices)\n",
    "    \n",
    "\n",
    "    # iterate over data\n",
    "    def next(self):\n",
    "        \n",
    "        def chunks(iterable, batch_size, trim_incomplete_batches=True):\n",
    "            n = max(1, batch_size)\n",
    "            end = len(iterable)/n*n if trim_incomplete_batches else len(iterable)\n",
    "            return (iterable[i:i+n] for i in xrange(0, end, n))\n",
    "        \n",
    "        for bucket_idx in xrange(self.num_buckets):\n",
    "            print(\"bucket_idx: %d\" % bucket_idx)\n",
    "            buck = self.bucketed_data[self.bucket_iterator_indices[bucket_idx]]\n",
    "            buck_len = len(buck[0])\n",
    "            buck_chx = chunks(range(buck_len, self.batch_size))\n",
    "            \n",
    "            try:\n",
    "                foo\n",
    "            except StopIteration as si:\n",
    "                if bucket_idx == self.num_\n",
    "#             for buck_chk in buck_chx:\n",
    "#                 src_ex = buck[0][buck_chk]\n",
    "#                 targ_ex = buck[1][buck_chk]\n",
    "#                 yield (src_ex, targ_ex)\n",
    "# #             if bucket_idx == self.num_buckets - 1:\n",
    "# #                 raise StopIteration\n",
    "#             return (src_ex, targ_ex)\n",
    "            \n",
    "        \n",
    "        # pick example from current group if not done\n",
    "        # if done, move to the next group \n",
    "        # if the end of the last group, raise StopError\n",
    "        \n",
    "        # raise StopIteration when done       \n",
    "\n",
    "    \n",
    "# From my iterator:\n",
    "\n",
    "# def iterate_groups(groups, batch_size=32):\n",
    "    \n",
    "#     def chunks(l, n, trim_incomplete_batches=True):\n",
    "#         n = max(1, n)\n",
    "#         end = len(l)/n*n if trim_incomplete_batches else len(l)\n",
    "#         return (l[i:i+n] for i in xrange(0, end, n))\n",
    "    \n",
    "#     for key, group in groups.items():\n",
    "#         num_examples = len(group[0])\n",
    "#         indices = list(xrange(num_examples))\n",
    "#         shuffle(indices)\n",
    "#         src_sent, targ_sent = group\n",
    "#         src_sent = src_sent[indices]\n",
    "#         targ_sent = targ_sent[indices]        \n",
    "#         for chunk in chunks(list(xrange(num_examples)), batch_size):\n",
    "#             yield [src_sent[chunk], targ_sent[chunk]]\n",
    "    \n",
    "# From BucketSentenceIterator:    \n",
    "    \n",
    "#     def next(self):\n",
    "#         if self.curr_idx == len(self.idx):\n",
    "#             raise StopIteration\n",
    "#         i, j = self.idx[self.curr_idx]\n",
    "#         self.curr_idx += 1\n",
    "\n",
    "#         if self.major_axis == 1:\n",
    "#             data = self.nddata[i][j:j+self.batch_size].T\n",
    "#             label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "#         else:\n",
    "#             data = self.nddata[i][j:j+self.batch_size]\n",
    "#             label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "#         return DataBatch([data], [label], pad=0,\n",
    "#                          bucket_key=self.buckets[i],\n",
    "#                          provide_data=[(self.data_name, data.shape)],\n",
    "#                          provide_label=[(self.label_name, label.shape)])    \n",
    "    \n",
    "    @staticmethod \n",
    "    def filter_long_sent(src_sent, targ_sent, max_len):\n",
    "        result = filter(lambda x: len(x[0]) <= max_len and len(x[1]) <= max_len, zip(src_sent, targ_sent))\n",
    "        return zip(*result)\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_buckets(src_sent, targ_sent, filter_smaller_counts_than=None, max_sent_len=60, min_sent_len=1):\n",
    "        length_pairs = map(lambda x: (len(x[0]), len(x[1])), zip(src_sent, targ_sent))\n",
    "        counts = list(Counter(length_pairs).items())\n",
    "        c_sorted = sorted(counts, key=operator.itemgetter(0, 1))\n",
    "        buckets = [i[0] for i in c_sorted if i[1] >= filter_smaller_counts_than and \n",
    "                   (max_sent_len is None or i[0][0] <= max_sent_len) and\n",
    "                   (max_sent_len is None or i[0][1] <= max_sent_len) and\n",
    "                   (min_sent_len is None or i[0][0] >= min_sent_len) and\n",
    "                   (min_sent_len is None or i[0][1] >= min_sent_len)]\n",
    "        return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i1 = Seq2SeqIterator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[  163,     3, 11222, ...,   245,   130,    36],\n",
      "       [  205,    39,  1608, ...,  1047,  3384,    36],\n",
      "       [ 1512,  3060,     2, ...,     3,  1300,    36],\n",
      "       ..., \n",
      "       [13912,  2667,    73, ...,   130,  1679,    36],\n",
      "       [  205,   213,   230, ...,   966,  4501,    36],\n",
      "       [  394,   144,    24, ...,  4014,   427,   154]], dtype=int32), array([[ 811,   11,  103, ...,  244,  185,   30],\n",
      "       [ 169,  170,  924, ..., 6821, 1343,   30],\n",
      "       [ 310,  214,  678, ...,   47, 4607,   30],\n",
      "       ..., \n",
      "       [ 822,   11,  361, ...,  914, 7805,   30],\n",
      "       [  45, 3917,  337, ...,    4, 2591,   30],\n",
      "       [  69, 1242,  107, ...,   89,   90,   30]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "i1.reset()\n",
    "i1.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[11315],\n",
       "        [   36],\n",
       "        [   36],\n",
       "        [ 4688],\n",
       "        [14993],\n",
       "        [   36],\n",
       "        [   36],\n",
       "        [   36],\n",
       "        [17233],\n",
       "        [   36],\n",
       "        [ 9511]], dtype=int32),\n",
       " array([[  503, 29654,   835,   319,  1712,  3436,   161,   152,  3467,\n",
       "            66,  2577,    30],\n",
       "        [ 1112,    24,  4157,   148,   143,    66,     8,  1062,     4,\n",
       "            70,   469,    30],\n",
       "        [18936,  4189,    24,    78,    24,   802,   214,    24, 19004,\n",
       "            74,  8430,    30],\n",
       "        [  595,   784, 10958,     8,  3112,  1952,  4940,   273,  1954,\n",
       "            17,  1953,    30],\n",
       "        [   98, 25750,   653,   290,   103,  5544,  1221,    24,     8,\n",
       "         25750,  2827,    30],\n",
       "        [   37,   595,    11,    37,   285,  8571,    11,    37, 31222,\n",
       "           774,    30,    37],\n",
       "        [   97,   938,  3484,    70,   363,    19,    86,   617,    30,\n",
       "          1357,  1576,   100],\n",
       "        [   98,  2887,   214,   619,   120,  1763,     4,  2314,     4,\n",
       "            70,   632,    30],\n",
       "        [   45, 21775,    41, 12096,   230,     8, 10976,   918,   337,\n",
       "             8,  4745,    30],\n",
       "        [   45,   539,    70,  1666,   337,   347,   388,  4275,    24,\n",
       "            70,   632,    30],\n",
       "        [   97,   938,  3484,    70,   363,    19,    86,  1575,    30,\n",
       "          8983,  1576,   100]], dtype=int32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1.reset()\n",
    "i1.bucketed_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src_sent = dataset.src_sent\n",
    "targ_sent = dataset.targ_sent\n",
    "\n",
    "sent_len = lambda x: map(lambda y: len(y), x)\n",
    "max_len = lambda x: max(sent_len(x))\n",
    "min_len = lambda x: min(sent_len(x))\n",
    "\n",
    "min_len = min(min(sent_len(src_sent)), min(sent_len(targ_sent)))\n",
    "# max_len = max(max(sent_len(src_sent)), max(sent_len(targ_sent)))\n",
    "\n",
    "# min_len = min\n",
    "max_len = 65\n",
    "increment = 5\n",
    "\n",
    "all_pairs = [(i, j) for i in xrange(\n",
    "        min_len,max_len+increment,increment\n",
    "    ) for j in xrange(\n",
    "        min_len,max_len+increment,increment\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i2 = Seq2SeqIterator(dataset, buckets=all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src, targ = i1.bucketed_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "indices = np.array(range(len(src)))\n",
    "np.random.shuffle(indices)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src, targ = i1.bucketed_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "targ[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
