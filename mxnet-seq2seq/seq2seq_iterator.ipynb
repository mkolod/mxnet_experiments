{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-04-03 14:51:36.236975. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from collections import namedtuple, Counter\n",
    "from unidecode import unidecode\n",
    "from itertools import groupby\n",
    "from mxnet.io import DataBatch, DataIter\n",
    "from random import shuffle\n",
    "from mxnet import ndarray\n",
    "\n",
    "# import deepdish as dd\n",
    "\n",
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get rid of annoying Python deprecation warnings from built-in JSON encoder\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Decode text as UTF-8\n",
    "# Remove diacritical signs and convert to Latin alphabet\n",
    "# Separate punctuation as separate \"words\"\n",
    "def tokenize_text(fname, vocab=None, invalid_label=0, start_label=1, sep_punctuation=True):\n",
    "    lines = unidecode(open(fname).read().decode('utf-8')).split('\\n')\n",
    "#     lines = [x for x in lines if x]\n",
    "    lines = map(lambda x: re.findall(r\"\\w+|[^\\w\\s]\", x, re.UNICODE), lines)    \n",
    "    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label, start_label=start_label)\n",
    "    return sentences, vocab\n",
    "\n",
    "Dataset = namedtuple(\n",
    "    'Dataset', \n",
    "    ['src_sent', 'src_vocab', 'inv_src_vocab', 'targ_sent', 'targ_vocab', 'inv_targ_vocab'])\n",
    "\n",
    "def invert_dict(d):\n",
    "    return {v: k for k, v in d.iteritems()}\n",
    "\n",
    "\n",
    "def get_data(src_path, targ_path, start_label=1, invalid_label=0, pad_symbol='<PAD>'):\n",
    "    src_sent, src_vocab = tokenize_text(src_path, start_label=start_label,\n",
    "                                invalid_label=invalid_label)\n",
    "    \n",
    "    src_vocab[pad_symbol] = invalid_label\n",
    "    inv_src_vocab = invert_dict(src_vocab)\n",
    "\n",
    "    targ_sent, targ_vocab = tokenize_text(targ_path, start_label=start_label, #new_start+1,\n",
    "                                          invalid_label=invalid_label)\n",
    "    \n",
    "    targ_vocab[pad_symbol] = invalid_label\n",
    "    inv_targ_vocab = invert_dict(targ_vocab)\n",
    "    \n",
    "    return Dataset(\n",
    "        src_sent=src_sent, src_vocab=src_vocab, inv_src_vocab=inv_src_vocab,\n",
    "        targ_sent=targ_sent, targ_vocab=targ_vocab, inv_targ_vocab=inv_targ_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def persist_dataset(dataset, path):\n",
    "    with open(path, 'wb+') as fileobj:\n",
    "        pickle.dump(dataset, fileobj)\n",
    "        \n",
    "def load_dataset(path):\n",
    "    with open(path, 'rb') as fileobj:\n",
    "        return pickle.load(fileobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = \\\n",
    "    get_data(\n",
    "        src_path='./data/europarl-v7.es-en.en_small',\n",
    "        targ_path='./data/europarl-v7.es-en.es_small',\n",
    "        start_label=1,\n",
    "        invalid_label=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Don't convert from NumPy to MxNet NDArray at the last minute.\n",
    "# Use MxNet NDArray from the beginning. This will require some changes\n",
    "# though, e.g. MxNet NDArray doesn't allow a len() call (use shape), etc.\n",
    "\n",
    "class Seq2SeqIter(DataIter):\n",
    "\n",
    "    class TwoDBisect:\n",
    "        def __init__(self, buckets):\n",
    "            self.buckets = sorted(buckets, key=operator.itemgetter(0, 1))\n",
    "            self.x, self.y = zip(*buckets)\n",
    "            self.x, self.y = np.array(list(self.x)), np.array(list(self.y))\n",
    "\n",
    "        def twod_bisect(self, source, target):    \n",
    "            offset1 = np.searchsorted(self.x, len(source), side='left')\n",
    "            offset2 = np.where(self.y[offset1:] >= len(target))[0]        \n",
    "            return self.buckets[offset1 + offset2[0]]     \n",
    "    \n",
    "    def __init__(\n",
    "        self, dataset, buckets=None, batch_size=32, max_sent_len=None,\n",
    "        data_name='data', label_name='softmax_label', dtype=np.int32, layout='NTC'):\n",
    "        self.data_name = data_name\n",
    "        self.label_name = label_name\n",
    "        self.dtype = dtype\n",
    "        self.layout = layout\n",
    "        self.batch_size = batch_size\n",
    "        self.src_sent = dataset.src_sent\n",
    "        self.targ_sent = dataset.targ_sent\n",
    "        if buckets:\n",
    "            z = zip(*buckets)\n",
    "            self.max_sent_len = max(max(z[0]), max(z[1]))\n",
    "        else:\n",
    "            self.max_sent_len = max_sent_len\n",
    "        if self.max_sent_len:\n",
    "            self.src_sent, self.targ_sent = self.filter_long_sent(\n",
    "                self.src_sent, self.targ_sent, self.max_sent_len) \n",
    "        self.src_vocab = dataset.src_vocab\n",
    "        self.targ_vocab = dataset.targ_vocab\n",
    "        self.inv_src_vocab = dataset.inv_src_vocab\n",
    "        self.inv_targ_vocab = dataset.inv_targ_vocab\n",
    "        # Can't filter smaller counts per bucket if those sentences still exist!\n",
    "        self.buckets = buckets if buckets else self.gen_buckets(\n",
    "            self.src_sent, self.targ_sent, filter_smaller_counts_than=1, max_sent_len=max_sent_len)\n",
    "        self.bisect = Seq2SeqIter.TwoDBisect(self.buckets)\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.pad_id = self.src_vocab['<PAD>']\n",
    "        # After bucketization, we should probably del self.src_sent and self.targ_sent\n",
    "        # to free up memory.\n",
    "        self.bucketed_data, self.bucket_idx_to_key = self.bucketize()\n",
    "        self.bucket_key_to_idx = invert_dict(dict(enumerate(self.bucket_idx_to_key)))\n",
    "        self.interbucket_idx = -1\n",
    "        self.curr_bucket_id = None\n",
    "        self.curr_chunks = None\n",
    "        self.curr_buck = None\n",
    "        self.switch_bucket = True\n",
    "        self.num_buckets = len(self.bucket_idx_to_key)\n",
    "        self.bucket_iterator_indices = list(range(self.num_buckets))\n",
    "\n",
    "    \n",
    "    def bucketize(self):\n",
    "        tuples = []\n",
    "        ctr = 0\n",
    "        for src, targ in zip(self.src_sent, self.targ_sent):\n",
    "            len_tup = self.bisect.twod_bisect(src, targ)\n",
    "            rev_src = src[::-1] \n",
    "            tuples.append((src, targ, len_tup))\n",
    "            \n",
    "        sorted_tuples = sorted(tuples, key=operator.itemgetter(2))\n",
    "        grouped = groupby(sorted_tuples, lambda x: x[2])\n",
    "        bucketed_data = [] \n",
    "        bucket_idx_to_key = []\n",
    "        \n",
    "        for group in grouped:\n",
    "            \n",
    "            # get src and targ sentences, ignore the last elem of the tuple \n",
    "            # (the grouping key of (src_len, targ_len))\n",
    "            key, value = group[0], map(lambda x: x[:2], group[1])\n",
    "            if len(value) < self.batch_size:\n",
    "                continue\n",
    "\n",
    "            # create padded representation\n",
    "            new_src = np.full((len(value), key[0]), self.pad_id, dtype=self.dtype)\n",
    "            new_targ = np.full((len(value), key[1]), self.pad_id, dtype=self.dtype)\n",
    "            \n",
    "            for idx, example in enumerate(value):\n",
    "                curr_src, curr_targ = example\n",
    "                rev_src = curr_src[::-1]\n",
    "                new_src[idx, :-(len(rev_src)+1):-1] = curr_src\n",
    "                new_targ[idx, :len(curr_targ)] = curr_targ\n",
    "                            \n",
    "            bucketed_data.append((new_src, new_targ))\n",
    "\n",
    "            bucket_idx_to_key.append(key)\n",
    "        return bucketed_data, bucket_idx_to_key\n",
    "    \n",
    "    def current_bucket_key(self):\n",
    "        return self.bucket_idx_to_key[self.interbucket_idx]\n",
    "    \n",
    "    def current_bucket_index(self):\n",
    "        return self.bucket_iterator_indices[self.interbucket_idx]\n",
    "\n",
    "    # shuffle the data within buckets, and reset iterator\n",
    "    def reset(self):\n",
    "        self.interbucket_idx = -1\n",
    "        for idx in xrange(len(self.bucketed_data)):\n",
    "            current = self.bucketed_data[idx]\n",
    "            src, targ = current\n",
    "            indices = np.array(range(src.shape[0]))\n",
    "            np.random.shuffle(indices)\n",
    "            src = src[indices]\n",
    "            targ = targ[indices]\n",
    "            self.bucketed_data[idx] = (src, targ)\n",
    "        shuffle(self.bucket_iterator_indices)\n",
    "\n",
    "    # iterate over data\n",
    "    def next(self):\n",
    "        try:\n",
    "            if self.switch_bucket:\n",
    "                self.interbucket_idx += 1\n",
    "                self.curr_bucket_id = self.bucket_iterator_indices[self.interbucket_idx]\n",
    "                self.curr_buck = self.bucketed_data[self.curr_bucket_id]\n",
    "                src_buck_len, src_buck_wid = self.curr_buck[0].shape\n",
    "                targ_buck_len, targ_buck_wid = self.curr_buck[1].shape                 \n",
    "                if src_buck_len == 0 or src_buck_wid == 0:\n",
    "                    raise StopIteration\n",
    "                if targ_buck_len == 0 or targ_buck_wid == 0:\n",
    "                    raise StopIteration\n",
    "                self.curr_chunks = self.chunks(range(src_buck_len), self.batch_size)\n",
    "                self.switch_bucket = False\n",
    "            current = self.curr_chunks.next()\n",
    "            src_ex = ndarray.array(self.curr_buck[0][current])\n",
    "            targ_ex = ndarray.array(self.curr_buck[1][current])\n",
    "            \n",
    "            return DataBatch([src_ex], [targ_ex], pad=0,\n",
    "                             bucket_key=self.bucket_idx_to_key[self.curr_bucket_id],\n",
    "                             provide_data=[(self.data_name, src_ex.shape)],\n",
    "                             provide_label=[(self.label_name, targ_ex.shape)])\n",
    "                \n",
    "        except StopIteration as si:\n",
    "            if self.interbucket_idx == self.num_buckets - 1:\n",
    "                self.reset()\n",
    "                self.switch_bucket = True\n",
    "                raise si\n",
    "            else:\n",
    "                self.switch_bucket = True\n",
    "                return self.next()\n",
    "\n",
    "    @staticmethod\n",
    "    def chunks(iterable, batch_size, trim_incomplete_batches=True):\n",
    "        n = max(1, batch_size)\n",
    "        end = len(iterable)/n*n if trim_incomplete_batches else len(iterable)\n",
    "        return (iterable[i:i+n] for i in xrange(0, end, n))\n",
    "    \n",
    "    @staticmethod \n",
    "    def filter_long_sent(src_sent, targ_sent, max_len):\n",
    "        result = filter(lambda x: len(x[0]) <= max_len and len(x[1]) <= max_len, zip(src_sent, targ_sent))\n",
    "        return zip(*result)\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_buckets(src_sent, targ_sent, filter_smaller_counts_than=None, max_sent_len=60, min_sent_len=1):\n",
    "        length_pairs = map(lambda x: (len(x[0]), len(x[1])), zip(src_sent, targ_sent))\n",
    "        counts = list(Counter(length_pairs).items())\n",
    "        c_sorted = sorted(counts, key=operator.itemgetter(0, 1))\n",
    "        buckets = [i[0] for i in c_sorted if i[1] >= filter_smaller_counts_than and \n",
    "                   (max_sent_len is None or i[0][0] <= max_sent_len) and\n",
    "                   (max_sent_len is None or i[0][1] <= max_sent_len) and\n",
    "                   (min_sent_len is None or i[0][0] >= min_sent_len) and\n",
    "                   (min_sent_len is None or i[0][1] >= min_sent_len)]\n",
    "        return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src_sent = dataset.src_sent\n",
    "targ_sent = dataset.targ_sent\n",
    "\n",
    "sent_len = lambda x: map(lambda y: len(y), x)\n",
    "max_len = lambda x: max(sent_len(x))\n",
    "min_len = lambda x: min(sent_len(x))\n",
    "\n",
    "min_len = min(min(sent_len(src_sent)), min(sent_len(targ_sent)))\n",
    "\n",
    "# min_len = min\n",
    "max_len = 65\n",
    "increment = 5\n",
    "\n",
    "all_pairs = [(i, j) for i in xrange(\n",
    "        min_len,max_len+increment,increment\n",
    "    ) for j in xrange(\n",
    "        min_len,max_len+increment,increment\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i2 = Seq2SeqIter(dataset, buckets=all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<NDArray 105x0 @cpu(0)>, <NDArray 105x5 @cpu(0)>),\n",
       " (<NDArray 2805x5 @cpu(0)>, <NDArray 2805x5 @cpu(0)>),\n",
       " (<NDArray 840x5 @cpu(0)>, <NDArray 840x10 @cpu(0)>),\n",
       " (<NDArray 43x5 @cpu(0)>, <NDArray 43x15 @cpu(0)>),\n",
       " (<NDArray 85x10 @cpu(0)>, <NDArray 85x0 @cpu(0)>),\n",
       " (<NDArray 1260x10 @cpu(0)>, <NDArray 1260x5 @cpu(0)>),\n",
       " (<NDArray 10568x10 @cpu(0)>, <NDArray 10568x10 @cpu(0)>),\n",
       " (<NDArray 3433x10 @cpu(0)>, <NDArray 3433x15 @cpu(0)>),\n",
       " (<NDArray 228x10 @cpu(0)>, <NDArray 228x20 @cpu(0)>),\n",
       " (<NDArray 108x15 @cpu(0)>, <NDArray 108x0 @cpu(0)>),\n",
       " (<NDArray 44x15 @cpu(0)>, <NDArray 44x5 @cpu(0)>),\n",
       " (<NDArray 3513x15 @cpu(0)>, <NDArray 3513x10 @cpu(0)>),\n",
       " (<NDArray 13245x15 @cpu(0)>, <NDArray 13245x15 @cpu(0)>),\n",
       " (<NDArray 5489x15 @cpu(0)>, <NDArray 5489x20 @cpu(0)>),\n",
       " (<NDArray 778x15 @cpu(0)>, <NDArray 778x25 @cpu(0)>),\n",
       " (<NDArray 88x15 @cpu(0)>, <NDArray 88x30 @cpu(0)>),\n",
       " (<NDArray 85x20 @cpu(0)>, <NDArray 85x0 @cpu(0)>),\n",
       " (<NDArray 299x20 @cpu(0)>, <NDArray 299x10 @cpu(0)>),\n",
       " (<NDArray 4758x20 @cpu(0)>, <NDArray 4758x15 @cpu(0)>),\n",
       " (<NDArray 12373x20 @cpu(0)>, <NDArray 12373x20 @cpu(0)>),\n",
       " (<NDArray 6495x20 @cpu(0)>, <NDArray 6495x25 @cpu(0)>),\n",
       " (<NDArray 1214x20 @cpu(0)>, <NDArray 1214x30 @cpu(0)>),\n",
       " (<NDArray 170x20 @cpu(0)>, <NDArray 170x35 @cpu(0)>),\n",
       " (<NDArray 59x25 @cpu(0)>, <NDArray 59x0 @cpu(0)>),\n",
       " (<NDArray 32x25 @cpu(0)>, <NDArray 32x10 @cpu(0)>),\n",
       " (<NDArray 629x25 @cpu(0)>, <NDArray 629x15 @cpu(0)>),\n",
       " (<NDArray 5031x25 @cpu(0)>, <NDArray 5031x20 @cpu(0)>),\n",
       " (<NDArray 10846x25 @cpu(0)>, <NDArray 10846x25 @cpu(0)>),\n",
       " (<NDArray 6316x25 @cpu(0)>, <NDArray 6316x30 @cpu(0)>),\n",
       " (<NDArray 1552x25 @cpu(0)>, <NDArray 1552x35 @cpu(0)>),\n",
       " (<NDArray 274x25 @cpu(0)>, <NDArray 274x40 @cpu(0)>),\n",
       " (<NDArray 47x25 @cpu(0)>, <NDArray 47x45 @cpu(0)>),\n",
       " (<NDArray 95x30 @cpu(0)>, <NDArray 95x15 @cpu(0)>),\n",
       " (<NDArray 861x30 @cpu(0)>, <NDArray 861x20 @cpu(0)>),\n",
       " (<NDArray 4410x30 @cpu(0)>, <NDArray 4410x25 @cpu(0)>),\n",
       " (<NDArray 8424x30 @cpu(0)>, <NDArray 8424x30 @cpu(0)>),\n",
       " (<NDArray 5542x30 @cpu(0)>, <NDArray 5542x35 @cpu(0)>),\n",
       " (<NDArray 1744x30 @cpu(0)>, <NDArray 1744x40 @cpu(0)>),\n",
       " (<NDArray 336x30 @cpu(0)>, <NDArray 336x45 @cpu(0)>),\n",
       " (<NDArray 67x30 @cpu(0)>, <NDArray 67x50 @cpu(0)>),\n",
       " (<NDArray 125x35 @cpu(0)>, <NDArray 125x20 @cpu(0)>),\n",
       " (<NDArray 870x35 @cpu(0)>, <NDArray 870x25 @cpu(0)>),\n",
       " (<NDArray 3591x35 @cpu(0)>, <NDArray 3591x30 @cpu(0)>),\n",
       " (<NDArray 6186x35 @cpu(0)>, <NDArray 6186x35 @cpu(0)>),\n",
       " (<NDArray 4322x35 @cpu(0)>, <NDArray 4322x40 @cpu(0)>),\n",
       " (<NDArray 1539x35 @cpu(0)>, <NDArray 1539x45 @cpu(0)>),\n",
       " (<NDArray 395x35 @cpu(0)>, <NDArray 395x50 @cpu(0)>),\n",
       " (<NDArray 63x35 @cpu(0)>, <NDArray 63x55 @cpu(0)>),\n",
       " (<NDArray 166x40 @cpu(0)>, <NDArray 166x25 @cpu(0)>),\n",
       " (<NDArray 803x40 @cpu(0)>, <NDArray 803x30 @cpu(0)>),\n",
       " (<NDArray 2743x40 @cpu(0)>, <NDArray 2743x35 @cpu(0)>),\n",
       " (<NDArray 4498x40 @cpu(0)>, <NDArray 4498x40 @cpu(0)>),\n",
       " (<NDArray 3148x40 @cpu(0)>, <NDArray 3148x45 @cpu(0)>),\n",
       " (<NDArray 1272x40 @cpu(0)>, <NDArray 1272x50 @cpu(0)>),\n",
       " (<NDArray 362x40 @cpu(0)>, <NDArray 362x55 @cpu(0)>),\n",
       " (<NDArray 92x40 @cpu(0)>, <NDArray 92x60 @cpu(0)>),\n",
       " (<NDArray 47x45 @cpu(0)>, <NDArray 47x25 @cpu(0)>),\n",
       " (<NDArray 129x45 @cpu(0)>, <NDArray 129x30 @cpu(0)>),\n",
       " (<NDArray 712x45 @cpu(0)>, <NDArray 712x35 @cpu(0)>),\n",
       " (<NDArray 1997x45 @cpu(0)>, <NDArray 1997x40 @cpu(0)>),\n",
       " (<NDArray 2904x45 @cpu(0)>, <NDArray 2904x45 @cpu(0)>),\n",
       " (<NDArray 2295x45 @cpu(0)>, <NDArray 2295x50 @cpu(0)>),\n",
       " (<NDArray 1018x45 @cpu(0)>, <NDArray 1018x55 @cpu(0)>),\n",
       " (<NDArray 341x45 @cpu(0)>, <NDArray 341x60 @cpu(0)>),\n",
       " (<NDArray 84x45 @cpu(0)>, <NDArray 84x65 @cpu(0)>),\n",
       " (<NDArray 36x50 @cpu(0)>, <NDArray 36x30 @cpu(0)>),\n",
       " (<NDArray 160x50 @cpu(0)>, <NDArray 160x35 @cpu(0)>),\n",
       " (<NDArray 568x50 @cpu(0)>, <NDArray 568x40 @cpu(0)>),\n",
       " (<NDArray 1415x50 @cpu(0)>, <NDArray 1415x45 @cpu(0)>),\n",
       " (<NDArray 2038x50 @cpu(0)>, <NDArray 2038x50 @cpu(0)>),\n",
       " (<NDArray 1593x50 @cpu(0)>, <NDArray 1593x55 @cpu(0)>),\n",
       " (<NDArray 829x50 @cpu(0)>, <NDArray 829x60 @cpu(0)>),\n",
       " (<NDArray 276x50 @cpu(0)>, <NDArray 276x65 @cpu(0)>),\n",
       " (<NDArray 48x55 @cpu(0)>, <NDArray 48x35 @cpu(0)>),\n",
       " (<NDArray 124x55 @cpu(0)>, <NDArray 124x40 @cpu(0)>),\n",
       " (<NDArray 406x55 @cpu(0)>, <NDArray 406x45 @cpu(0)>),\n",
       " (<NDArray 995x55 @cpu(0)>, <NDArray 995x50 @cpu(0)>),\n",
       " (<NDArray 1379x55 @cpu(0)>, <NDArray 1379x55 @cpu(0)>),\n",
       " (<NDArray 1183x55 @cpu(0)>, <NDArray 1183x60 @cpu(0)>),\n",
       " (<NDArray 600x55 @cpu(0)>, <NDArray 600x65 @cpu(0)>),\n",
       " (<NDArray 45x60 @cpu(0)>, <NDArray 45x40 @cpu(0)>),\n",
       " (<NDArray 128x60 @cpu(0)>, <NDArray 128x45 @cpu(0)>),\n",
       " (<NDArray 320x60 @cpu(0)>, <NDArray 320x50 @cpu(0)>),\n",
       " (<NDArray 692x60 @cpu(0)>, <NDArray 692x55 @cpu(0)>),\n",
       " (<NDArray 982x60 @cpu(0)>, <NDArray 982x60 @cpu(0)>),\n",
       " (<NDArray 773x60 @cpu(0)>, <NDArray 773x65 @cpu(0)>),\n",
       " (<NDArray 105x65 @cpu(0)>, <NDArray 105x50 @cpu(0)>),\n",
       " (<NDArray 226x65 @cpu(0)>, <NDArray 226x55 @cpu(0)>),\n",
       " (<NDArray 483x65 @cpu(0)>, <NDArray 483x60 @cpu(0)>),\n",
       " (<NDArray 649x65 @cpu(0)>, <NDArray 649x65 @cpu(0)>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2.bucketed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.14574642  0.70912112  0.24229097  0.68700416 -0.6091984 ]\n",
      " [ 0.17276195 -0.32644844  0.68268244  0.58893091  0.22677232]\n",
      " [-0.95003072  0.22062339 -0.25506202 -0.24040491 -0.36923575]\n",
      " [-0.52928886  0.96218995  0.66630804  0.25313837 -0.79620362]\n",
      " [ 1.25394634 -0.26037199 -0.49700556 -1.53185457  2.6311021 ]]\n",
      "[[-1.14574647  0.70912111  0.24229097  0.68700415 -0.60919839]\n",
      " [ 0.17276195 -0.32644844  0.68268245  0.5889309   0.22677232]\n",
      " [-0.95003074  0.22062339 -0.25506201 -0.2404049  -0.36923575]\n",
      " [-0.52928889  0.96218997  0.66630805  0.25313836 -0.79620361]\n",
      " [ 1.2539463  -0.26037198 -0.49700555 -1.53185463  2.63110209]]\n"
     ]
    }
   ],
   "source": [
    "narr = np.random.randn(5, 5)\n",
    "print(narr)\n",
    "marr = mx.nd.array(narr).asnumpy()\n",
    "print(marr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_text(iterator, vocab, text, max_examples=10):\n",
    "    for i in range(min(max_examples, len(src))):\n",
    "        x = text[i]\n",
    "        s = []\n",
    "        for j in range(len(x)):\n",
    "            s.append(vocab[x[j]])\n",
    "        print(\" \".join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NDArray' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-246ac127d10c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_src_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-ef03f711c52a>\u001b[0m in \u001b[0;36mprint_text\u001b[0;34m(iterator, vocab, text, max_examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NDArray' has no len()"
     ]
    }
   ],
   "source": [
    "print_text(i2, i2.inv_src_vocab, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.nd.array(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamento el rechazo de la propuesta destinada a presentar una resolucion del Parlamento Europeo .\n",
      "Juzgaremos a ambas instituciones por sus actos , no por sus palabras . <PAD> <PAD>\n",
      "Espero que la Comision y los paises miembros se interesen realmente en este asunto .\n",
      "Lo lamento , no obstante , muchos le han escuchado con atencion . <PAD> <PAD>\n",
      "De repente dijo : \" ? Ven este vaso que tengo aqui delante ? \"\n",
      ". ( FR ) Senor Presidente , Grecia sera el duodecimo miembro del euro .\n",
      "Me dicen que hay un numero limitado de digitos para escribirlo . <PAD> <PAD> <PAD>\n",
      "Pero no creemos que la lucha contra la exclusion social pueda fragmentarse . <PAD> <PAD>\n",
      "Pasamos una segunda etapa durante la cual se consagraron los modelos de mujer . <PAD>\n",
      "El unico problema es que ni siquiera dispone de cubos para acarrear el agua .\n"
     ]
    }
   ],
   "source": [
    "print_text(i2, i2.inv_targ_vocab, targ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
