{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-30 03:17:46.978407. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from collections import namedtuple, Counter\n",
    "from unidecode import unidecode\n",
    "from itertools import groupby\n",
    "from mxnet.io import DataIter\n",
    "from random import shuffle\n",
    "\n",
    "import deepdish as dd\n",
    "\n",
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get rid of annoying Python deprecation warnings from built-in JSON encoder\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Decode text as UTF-8\n",
    "# Remove diacritical signs and convert to Latin alphabet\n",
    "# Separate punctuation as separate \"words\"\n",
    "def tokenize_text(fname, vocab=None, invalid_label=0, start_label=1, sep_punctuation=True):\n",
    "    lines = unidecode(open(fname).read().decode('utf-8')).split('\\n')\n",
    "    lines = [x for x in lines if x]\n",
    "    lines = map(lambda x: re.findall(r\"\\w+|[^\\w\\s]\", x, re.UNICODE), lines)    \n",
    "    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label, start_label=start_label)\n",
    "    return sentences, vocab\n",
    "\n",
    "Dataset = namedtuple(\n",
    "    'Dataset', \n",
    "    ['src_sent', 'src_vocab', 'inv_src_vocab', 'targ_sent', 'targ_vocab', 'inv_targ_vocab'])\n",
    "\n",
    "def invert_dict(d):\n",
    "    return {v: k for k, v in d.iteritems()}\n",
    "\n",
    "\n",
    "def get_data(src_path, targ_path, start_label=1, invalid_label=0, pad_symbol='<PAD>'):\n",
    "    src_sent, src_vocab = tokenize_text(src_path, start_label=start_label,\n",
    "                                invalid_label=invalid_label)\n",
    "    \n",
    "    src_vocab[pad_symbol] = invalid_label\n",
    "    inv_src_vocab = invert_dict(src_vocab)\n",
    "\n",
    "    targ_sent, targ_vocab = tokenize_text(targ_path, start_label=start_label, #new_start+1,\n",
    "                                          invalid_label=invalid_label)\n",
    "    \n",
    "    targ_vocab[pad_symbol] = invalid_label\n",
    "    inv_targ_vocab = invert_dict(targ_vocab)\n",
    "    \n",
    "    return Dataset(\n",
    "        src_sent=src_sent, src_vocab=src_vocab, inv_src_vocab=inv_src_vocab,\n",
    "        targ_sent=targ_sent, targ_vocab=targ_vocab, inv_targ_vocab=inv_targ_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def persist_dataset(dataset, path):\n",
    "    with open(path, 'wb+') as fileobj:\n",
    "        pickle.dump(dataset, fileobj)\n",
    "        \n",
    "def load_dataset(path):\n",
    "    with open(path, 'rb') as fileobj:\n",
    "        return pickle.load(fileobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = \\\n",
    "    get_data(\n",
    "        src_path='./data/europarl-v7.es-en.en_small',\n",
    "        targ_path='./data/europarl-v7.es-en.es_small',\n",
    "        start_label=1,\n",
    "        invalid_label=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TwoDBisect:\n",
    "    def __init__(self, buckets):\n",
    "        self.buckets = sorted(buckets, key=operator.itemgetter(0, 1))\n",
    "        self.x, self.y = zip(*buckets)\n",
    "        self.x, self.y = np.array(list(self.x)), np.array(list(self.y))\n",
    "\n",
    "    def twod_bisect(self, source, target):    \n",
    "        offset1 = np.searchsorted(self.x, len(source), side='left')\n",
    "        offset2 = np.where(self.y[offset1:] >= len(target))[0]        \n",
    "        return self.buckets[offset1 + offset2[0]] \n",
    "\n",
    "class Seq2SeqIterator:    \n",
    "    \n",
    "    def __init__(self, dataset, buckets=None, batch_size=32, max_sent_len=None):\n",
    "        self.src_sent = dataset.src_sent\n",
    "        self.targ_sent = dataset.targ_sent\n",
    "        # make this default to the maximum of the ???\n",
    "        if buckets:\n",
    "            z = zip(*buckets)\n",
    "            self.max_sent_len = max(z[0], z[1])\n",
    "            print(self.max_sent_len)\n",
    "        else:\n",
    "            self.max_sent_len = max_sent_len\n",
    "        if max_sent_len:\n",
    "            self.src_sent, self.targ_sent = self.filter_long_sent(self.src_sent, self.targ_sent, self.max_sent_len) \n",
    "        self.src_vocab = dataset.src_vocab\n",
    "        self.targ_vocab = dataset.targ_vocab\n",
    "        self.inv_src_vocab = dataset.inv_src_vocab\n",
    "        self.inv_targ_vocab = dataset.inv_targ_vocab\n",
    "        # Can't filter smaller counts per bucket if those sentences still exist!\n",
    "        self.buckets = buckets if buckets else self.gen_buckets(\n",
    "            self.src_sent, self.targ_sent, filter_smaller_counts_than=1, max_sent_len=max_sent_len)\n",
    "        self.bisect = TwoDBisect(self.buckets)\n",
    "        self.max_sent_len = max_sent_len\n",
    "    \n",
    "    def group_lengths(self):\n",
    "#         short_sentences = filter(lambda x: len(x[0]) <= max_len and len(x[1]) <= max_len, zip(self.src_sent, self.targ_sent))\n",
    "        tuples = []\n",
    "        ctr = 0\n",
    "        for src, targ in zip(self.src_sent, self.targ_sent):\n",
    "            try:\n",
    "#                 print(src)\n",
    "#                 print(targ)\n",
    "                len_tup = self.bisect.twod_bisect(src, targ)\n",
    "                rev_src = src[::-1] \n",
    "                tuples.append((src, targ, len_tup))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(src)\n",
    "                print(targ)\n",
    "                break\n",
    "        sorted_tuples = sorted(tuples, key=operator.itemgetter(2))\n",
    "\n",
    "        \n",
    "    @staticmethod \n",
    "    def filter_long_sent(src_sent, targ_sent, max_len):\n",
    "        return filter(lambda x: len(x[0]) <= max_len and len(x[1]) <= max_len, zip(src_sent, targ_sent))\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_buckets(src_sent, targ_sent, filter_smaller_counts_than=None, max_sent_len=60, min_sent_len=1):\n",
    "        length_pairs = map(lambda x: (len(x[0]), len(x[1])), zip(src_sent, targ_sent))\n",
    "        counts = list(Counter(length_pairs).items())\n",
    "        c_sorted = sorted(counts, key=operator.itemgetter(0, 1))\n",
    "        buckets = [i[0] for i in c_sorted if i[1] >= filter_smaller_counts_than and \n",
    "                   (max_sent_len is None or i[0][0] <= max_sent_len) and\n",
    "                   (max_sent_len is None or i[0][1] <= max_sent_len) and\n",
    "                   (min_sent_len is None or i[0][0] >= min_sent_len) and\n",
    "                   (min_sent_len is None or i[0][1] >= min_sent_len)]\n",
    "        return buckets\n",
    "\n",
    "#    def reset(self):\n",
    "#         self.curr_idx = 0\n",
    "#         random.shuffle(self.idx)\n",
    "#         for buck in self.data:\n",
    "#             np.random.shuffle(buck)\n",
    "\n",
    "#         self.nddata = []\n",
    "#         self.ndlabel = []\n",
    "#         for buck in self.data:\n",
    "#             label = np.empty_like(buck)\n",
    "#             label[:, :-1] = buck[:, 1:]\n",
    "#             label[:, -1] = self.invalid_label\n",
    "#             self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "#             self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "#     def next(self):\n",
    "#         if self.curr_idx == len(self.idx):\n",
    "#             raise StopIteration\n",
    "#         i, j = self.idx[self.curr_idx]\n",
    "#         self.curr_idx += 1\n",
    "\n",
    "#         if self.major_axis == 1:\n",
    "#             data = self.nddata[i][j:j+self.batch_size].T\n",
    "#             label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "#         else:\n",
    "#             data = self.nddata[i][j:j+self.batch_size]\n",
    "#             label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "#         return DataBatch([data], [label], pad=0,\n",
    "#                          bucket_key=self.buckets[i],\n",
    "#                          provide_data=[(self.data_name, data.shape)],\n",
    "#                          provide_label=[(self.label_name, label.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i1 = Seq2SeqIterator(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i1.group_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src_sent = dataset.src_sent\n",
    "targ_sent = dataset.targ_sent\n",
    "\n",
    "sent_len = lambda x: map(lambda y: len(y), x)\n",
    "max_len = lambda x: max(sent_len(x))\n",
    "min_len = lambda x: min(sent_len(x))\n",
    "\n",
    "min_len = min(min(sent_len(src_sent)), min(sent_len(targ_sent)))\n",
    "# max_len = max(max(sent_len(src_sent)), max(sent_len(targ_sent)))\n",
    "\n",
    "# min_len = min\n",
    "max_len = 65\n",
    "increment = 5\n",
    "\n",
    "all_pairs = [(i, j) for i in range(\n",
    "        min_len,max_len+increment,increment\n",
    "    ) for j in range(\n",
    "        min_len,max_len+increment,increment\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i2 = Seq2SeqIterator(dataset, all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[5, 6, 7, 3, 4, 2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 5, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3, 30, 31, 24, 32, 25, 33, 34, 35, 36]\n",
      "[6, 7, 8, 3, 4, 5, 2, 9, 10, 11, 12, 8, 13, 14, 4, 15, 16, 11, 17, 18, 19, 20, 21, 22, 23, 4, 24, 25, 26, 27, 28, 29, 30]\n",
      "[37, 16, 38, 24, 39, 40, 41, 16, 3, 42, 43, 44, 45, 43, 46, 22, 47, 16, 48, 3, 49, 29, 25, 50, 2, 51, 52, 25, 53, 2, 54, 55, 31, 56, 57, 58, 36]\n",
      "[31, 32, 33, 34, 35, 11, 8, 36, 37, 38, 2, 39, 40, 37, 41, 42, 43, 44, 30, 45, 46, 11, 47, 48, 4, 49, 4, 50, 51, 33, 52, 53, 4, 54, 55, 56, 57, 30]\n",
      "[59, 40, 60, 25, 61, 11, 62, 63, 29, 3, 64, 2, 3, 65, 66, 67, 16, 68, 62, 69, 70, 4, 36]\n",
      "[20, 21, 33, 58, 59, 60, 61, 8, 62, 63, 47, 64, 65, 11, 66, 8, 67, 4, 68, 3, 4, 5, 30]\n",
      "[71, 3, 72, 16, 5, 73, 19, 22, 74, 25, 75, 43, 76, 77, 16, 38, 25, 50, 2, 78, 40, 60, 16, 11, 79, 2, 80, 3, 81, 82, 16, 83, 84, 2, 3, 85, 86, 16, 29, 3, 87, 51, 2, 3, 8, 88, 36]\n",
      "[69, 70, 71, 4, 24, 42, 72, 11, 4, 73, 74, 75, 76, 24, 77, 78, 33, 79, 11, 80, 24, 81, 59, 82, 4, 83, 66, 84, 4, 85, 86, 53, 4, 86, 87, 11, 66, 47, 88, 51, 4, 70, 89, 90, 91, 30]\n",
      "[89, 90, 16, 91, 16, 92, 62, 75, 43, 76, 77, 36]\n",
      "[92, 19, 32, 19, 24, 93, 94, 4, 95, 63, 96, 59, 82, 4, 83, 30]\n",
      "[93, 94, 95, 96, 17, 97, 25, 75, 43, 76, 77, 98]\n",
      "[97, 98, 9, 11, 4, 95, 11, 99, 59, 82, 4, 83, 100]\n",
      "[99, 100, 16, 11, 25, 101, 2, 102, 36]\n",
      "[101, 102, 11, 103, 104, 4, 105, 30]\n",
      "[59, 39, 103, 104, 105, 3, 106, 17, 107, 31, 108, 40, 109, 25, 50, 2, 110, 111, 17, 112, 29, 113, 114, 36]\n",
      "[106, 107, 108, 70, 109, 17, 70, 110, 24, 42, 33, 44, 103, 111, 4, 112, 17, 113, 66, 114, 115, 30]\n",
      "[115, 2, 3, 49, 116, 117, 118, 29, 113, 114, 119, 120, 121, 122, 16, 123, 124, 125, 3, 8, 9, 126, 25, 66, 127, 128, 36]\n",
      "[116, 4, 86, 117, 24, 118, 33, 119, 66, 114, 115, 43, 52, 120, 121, 30, 122, 123, 11, 124, 125, 126, 127, 128, 8, 9, 10, 30]\n",
      "[129, 130, 103, 131, 92, 24, 16, 99, 100, 16, 22, 132, 25, 133, 22, 3, 113, 134, 100, 135, 9, 43, 76, 136, 137, 138, 17, 3, 139, 140, 141, 29, 113, 114, 17, 142, 143, 22, 144, 145, 146, 147, 148, 22, 149, 25, 150, 151, 22, 25, 117, 152, 153, 154]\n",
      "[129, 130, 131, 24, 107, 11, 101, 102, 11, 132, 103, 133, 120, 134, 4, 114, 115, 135, 86, 136, 2, 9, 108, 137, 17, 138, 139, 140, 11, 141, 24, 142, 143, 78, 144, 63, 145, 103, 146, 147, 148, 70, 149, 150, 151, 24, 152, 153, 154, 155, 129]\n",
      "[155, 16, 120, 156, 16, 5, 157, 158, 159, 2, 3, 160, 24, 40, 126, 161, 18, 103, 162, 131, 36]\n",
      "[156, 11, 157, 158, 11, 159, 24, 103, 160, 161, 70, 24, 107, 162, 4, 163, 164, 165, 166, 30]\n",
      "[163, 3, 95, 164, 16, 5, 165, 144, 38, 120, 156, 166, 161, 36]\n",
      "[156, 70, 167, 152, 4, 73, 11, 168, 78, 24, 8, 157, 158, 162, 4, 163, 30]\n",
      "[99, 100, 16, 11, 25, 101, 2, 102, 36]\n",
      "[101, 102, 11, 103, 104, 4, 105, 30]\n",
      "[5, 18, 19, 167, 168, 169, 170, 171, 172, 173, 36]\n",
      "[169, 170, 24, 77, 171, 61, 8, 172, 173, 174, 19, 70, 175, 30]\n",
      "[174, 175, 176, 22, 177, 31, 39, 178, 179, 11, 180, 17, 181, 5, 39, 91, 182, 21, 36]\n",
      "[176, 177, 42, 178, 19, 59, 179, 2, 24, 42, 180, 8, 181, 11, 182, 24, 66, 183, 19, 184, 30]\n",
      "[94, 183, 184, 11, 185, 186, 187, 188, 189, 9, 11, 180, 17, 190, 25, 191, 29, 192, 193, 31, 25, 194, 2, 195, 196, 73, 103, 197, 92, 51, 181, 198, 22, 199, 200, 201, 202, 203, 204, 36]\n",
      "[98, 185, 186, 61, 47, 187, 4, 188, 189, 42, 190, 120, 9, 8, 181, 17, 191, 103, 192, 66, 8, 193, 194, 66, 195, 19, 103, 196, 4, 197, 198, 66, 199, 24, 200, 201, 19, 47, 51, 24, 41, 202, 203, 204, 205, 4, 206, 4, 207, 30]\n",
      "[205, 206, 31, 62, 73, 103, 207, 208, 3, 209, 2, 210, 211, 36]\n",
      "[98, 185, 208, 24, 42, 200, 201, 19, 209, 2, 210, 4, 211, 212, 30]\n",
      "[5, 212, 31, 3, 209, 2, 210, 211, 213, 25, 214, 215, 209, 2, 3, 216, 217, 218, 17, 25, 191, 22, 219, 130, 18, 103, 220, 221, 36]\n",
      "[213, 24, 8, 210, 4, 211, 212, 214, 59, 210, 215, 216, 4, 86, 217, 218, 219, 11, 108, 78, 24, 103, 192, 24, 78, 220, 214, 221, 222, 30]\n",
      "[5, 222, 22, 223, 224, 225, 148, 182, 158, 226, 2, 31, 227, 22, 228, 213, 229, 25, 184, 16, 230, 25, 231, 191, 16, 17, 224, 31, 213, 177, 5, 148, 232, 144, 11, 180, 36]\n",
      "[223, 224, 225, 42, 226, 227, 68, 228, 4, 229, 19, 78, 24, 230, 214, 59, 185, 11, 41, 103, 192, 231, 11, 17, 225, 214, 232, 24, 233, 234, 8, 181, 30]\n",
      "[233, 213, 234, 3, 235, 236, 24, 237, 16, 238, 24, 23, 16, 182, 62, 175, 16, 239, 36, 240, 36, 11, 180, 241, 22, 3, 242, 2, 3, 243, 2, 3, 184, 36]\n",
      "[235, 236, 11, 225, 237, 78, 238, 11, 239, 234, 152, 104, 66, 240, 241, 11, 214, 242, 11, 8, 181, 243, 4, 24, 42, 244, 8, 185, 30]\n",
      "[99, 100, 16, 244, 245, 62, 28, 43, 76, 246, 69, 70, 4, 2, 3, 8, 9, 16, 25, 247, 166, 109, 248, 16, 249, 92, 65, 180, 16, 29, 250, 29, 251, 16, 92, 3, 252, 2, 25, 253, 254, 28, 70, 255, 256, 123, 166, 109, 257, 22, 258, 36, 259, 165, 260, 261, 120, 262, 36]\n",
      "[101, 102, 11, 245, 74, 8, 246, 3, 247, 4, 5, 4, 68, 39, 2, 9, 10, 11, 248, 11, 66, 47, 249, 250, 11, 66, 251, 11, 42, 43, 252, 63, 8, 253, 181, 70, 254, 4, 59, 255, 19, 70, 256, 257, 11, 59, 258, 4, 259, 260, 24, 261, 74, 8, 262, 4, 263, 30]\n",
      "[263, 3, 264, 2, 25, 265, 266, 16, 120, 267, 16, 25, 268, 166, 269, 109, 270, 16, 181, 271, 49, 272, 16, 273, 274, 36, 275, 16, 5, 18, 276, 24, 16, 29, 277, 245, 3, 278, 181, 213, 279, 280, 281, 282, 3, 8, 9, 17, 282, 3, 283, 2, 3, 8, 284, 16, 22, 285, 286, 16, 287, 3, 288, 2, 167, 289, 290, 17, 3, 291, 24, 292, 16, 22, 3, 100, 17, 22, 3, 293, 2, 250, 16, 120, 294, 16, 123, 166, 3, 295, 22, 102, 25, 296, 2, 252, 17, 22, 297, 3, 298, 299, 36]\n",
      "[69, 264, 4, 59, 265, 266, 11, 8, 121, 30, 267, 11, 42, 43, 268, 103, 269, 11, 270, 271, 33, 52, 272, 17, 273, 47, 274, 77, 275, 11, 276, 277, 11, 4, 278, 74, 70, 279, 280, 281, 108, 8, 9, 10, 17, 108, 282, 70, 283, 284, 11, 285, 80, 24, 286, 74, 8, 287, 4, 154, 288, 17, 4, 70, 289, 24, 290, 148, 8, 134, 17, 8, 291, 2, 292, 4, 251, 11, 121, 30, 293, 11, 24, 294, 70, 295, 4, 296, 70, 297, 19, 298, 17, 4, 299, 120, 255, 30]\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "[263, 3, 264, 2, 25, 265, 266, 16, 120, 267, 16, 25, 268, 166, 269, 109, 270, 16, 181, 271, 49, 272, 16, 273, 274, 36, 275, 16, 5, 18, 276, 24, 16, 29, 277, 245, 3, 278, 181, 213, 279, 280, 281, 282, 3, 8, 9, 17, 282, 3, 283, 2, 3, 8, 284, 16, 22, 285, 286, 16, 287, 3, 288, 2, 167, 289, 290, 17, 3, 291, 24, 292, 16, 22, 3, 100, 17, 22, 3, 293, 2, 250, 16, 120, 294, 16, 123, 166, 3, 295, 22, 102, 25, 296, 2, 252, 17, 22, 297, 3, 298, 299, 36]\n",
      "[69, 264, 4, 59, 265, 266, 11, 8, 121, 30, 267, 11, 42, 43, 268, 103, 269, 11, 270, 271, 33, 52, 272, 17, 273, 47, 274, 77, 275, 11, 276, 277, 11, 4, 278, 74, 70, 279, 280, 281, 108, 8, 9, 10, 17, 108, 282, 70, 283, 284, 11, 285, 80, 24, 286, 74, 8, 287, 4, 154, 288, 17, 4, 70, 289, 24, 290, 148, 8, 134, 17, 8, 291, 2, 292, 4, 251, 11, 121, 30, 293, 11, 24, 294, 70, 295, 4, 296, 70, 297, 19, 298, 17, 4, 299, 120, 255, 30]\n"
     ]
    }
   ],
   "source": [
    "i2.group_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
