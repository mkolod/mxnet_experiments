{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-30 17:42:05.965210. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from collections import namedtuple, Counter\n",
    "from unidecode import unidecode\n",
    "from itertools import groupby\n",
    "from mxnet.io import DataIter\n",
    "from random import shuffle\n",
    "\n",
    "import deepdish as dd\n",
    "\n",
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get rid of annoying Python deprecation warnings from built-in JSON encoder\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Decode text as UTF-8\n",
    "# Remove diacritical signs and convert to Latin alphabet\n",
    "# Separate punctuation as separate \"words\"\n",
    "def tokenize_text(fname, vocab=None, invalid_label=0, start_label=1, sep_punctuation=True):\n",
    "    lines = unidecode(open(fname).read().decode('utf-8')).split('\\n')\n",
    "    lines = [x for x in lines if x]\n",
    "    lines = map(lambda x: re.findall(r\"\\w+|[^\\w\\s]\", x, re.UNICODE), lines)    \n",
    "    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label, start_label=start_label)\n",
    "    return sentences, vocab\n",
    "\n",
    "Dataset = namedtuple(\n",
    "    'Dataset', \n",
    "    ['src_sent', 'src_vocab', 'inv_src_vocab', 'targ_sent', 'targ_vocab', 'inv_targ_vocab'])\n",
    "\n",
    "def invert_dict(d):\n",
    "    return {v: k for k, v in d.iteritems()}\n",
    "\n",
    "\n",
    "def get_data(src_path, targ_path, start_label=1, invalid_label=0, pad_symbol='<PAD>'):\n",
    "    src_sent, src_vocab = tokenize_text(src_path, start_label=start_label,\n",
    "                                invalid_label=invalid_label)\n",
    "    \n",
    "    src_vocab[pad_symbol] = invalid_label\n",
    "    inv_src_vocab = invert_dict(src_vocab)\n",
    "\n",
    "    targ_sent, targ_vocab = tokenize_text(targ_path, start_label=start_label, #new_start+1,\n",
    "                                          invalid_label=invalid_label)\n",
    "    \n",
    "    targ_vocab[pad_symbol] = invalid_label\n",
    "    inv_targ_vocab = invert_dict(targ_vocab)\n",
    "    \n",
    "    return Dataset(\n",
    "        src_sent=src_sent, src_vocab=src_vocab, inv_src_vocab=inv_src_vocab,\n",
    "        targ_sent=targ_sent, targ_vocab=targ_vocab, inv_targ_vocab=inv_targ_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def persist_dataset(dataset, path):\n",
    "    with open(path, 'wb+') as fileobj:\n",
    "        pickle.dump(dataset, fileobj)\n",
    "        \n",
    "def load_dataset(path):\n",
    "    with open(path, 'rb') as fileobj:\n",
    "        return pickle.load(fileobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = \\\n",
    "    get_data(\n",
    "        src_path='./data/europarl-v7.es-en.en_small',\n",
    "        targ_path='./data/europarl-v7.es-en.es_small',\n",
    "        start_label=1,\n",
    "        invalid_label=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TwoDBisect:\n",
    "    def __init__(self, buckets):\n",
    "        self.buckets = sorted(buckets, key=operator.itemgetter(0, 1))\n",
    "        self.x, self.y = zip(*buckets)\n",
    "        self.x, self.y = np.array(list(self.x)), np.array(list(self.y))\n",
    "\n",
    "    def twod_bisect(self, source, target):    \n",
    "        offset1 = np.searchsorted(self.x, len(source), side='left')\n",
    "        offset2 = np.where(self.y[offset1:] >= len(target))[0]        \n",
    "        return self.buckets[offset1 + offset2[0]] \n",
    "\n",
    "class Seq2SeqIterator:    \n",
    "    \n",
    "    def __init__(self, dataset, buckets=None, batch_size=32, max_sent_len=None):\n",
    "        self.src_sent = dataset.src_sent\n",
    "        self.targ_sent = dataset.targ_sent\n",
    "        if buckets:\n",
    "            z = zip(*buckets)\n",
    "            self.max_sent_len = max(max(z[0]), max(z[1]))\n",
    "        else:\n",
    "            self.max_sent_len = max_sent_len\n",
    "        if self.max_sent_len:\n",
    "            self.src_sent, self.targ_sent = self.filter_long_sent(self.src_sent, self.targ_sent, self.max_sent_len) \n",
    "        self.src_vocab = dataset.src_vocab\n",
    "        self.targ_vocab = dataset.targ_vocab\n",
    "        self.inv_src_vocab = dataset.inv_src_vocab\n",
    "        self.inv_targ_vocab = dataset.inv_targ_vocab\n",
    "        # Can't filter smaller counts per bucket if those sentences still exist!\n",
    "        self.buckets = buckets if buckets else self.gen_buckets(\n",
    "            self.src_sent, self.targ_sent, filter_smaller_counts_than=1, max_sent_len=max_sent_len)\n",
    "        self.bisect = TwoDBisect(self.buckets)\n",
    "        self.max_sent_len = max_sent_len\n",
    "    \n",
    "    def group_lengths(self):\n",
    "        tuples = []\n",
    "        ctr = 0\n",
    "        for src, targ in zip(self.src_sent, self.targ_sent):\n",
    "            try:\n",
    "                len_tup = self.bisect.twod_bisect(src, targ)\n",
    "                rev_src = src[::-1] \n",
    "                tuples.append((src, targ, len_tup))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(src)\n",
    "                print(targ)\n",
    "                raise e\n",
    "        sorted_tuples = sorted(tuples, key=operator.itemgetter(2))\n",
    "        g = groupby(sorted_tuples, lambda x: x[2])\n",
    "        groups = {}\n",
    "        for i in g:\n",
    "            groups[i[0]] = map(lambda x: list(x[:2]), i[1])\n",
    "        print(groups[(11,11)])\n",
    "\n",
    "        \n",
    "    @staticmethod \n",
    "    def filter_long_sent(src_sent, targ_sent, max_len):\n",
    "        result = filter(lambda x: len(x[0]) <= max_len and len(x[1]) <= max_len, zip(src_sent, targ_sent))\n",
    "        return zip(*result)\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_buckets(src_sent, targ_sent, filter_smaller_counts_than=None, max_sent_len=60, min_sent_len=1):\n",
    "        length_pairs = map(lambda x: (len(x[0]), len(x[1])), zip(src_sent, targ_sent))\n",
    "        counts = list(Counter(length_pairs).items())\n",
    "        c_sorted = sorted(counts, key=operator.itemgetter(0, 1))\n",
    "        buckets = [i[0] for i in c_sorted if i[1] >= filter_smaller_counts_than and \n",
    "                   (max_sent_len is None or i[0][0] <= max_sent_len) and\n",
    "                   (max_sent_len is None or i[0][1] <= max_sent_len) and\n",
    "                   (min_sent_len is None or i[0][0] >= min_sent_len) and\n",
    "                   (min_sent_len is None or i[0][1] >= min_sent_len)]\n",
    "        return buckets\n",
    "\n",
    "#    def reset(self):\n",
    "#         self.curr_idx = 0\n",
    "#         random.shuffle(self.idx)\n",
    "#         for buck in self.data:\n",
    "#             np.random.shuffle(buck)\n",
    "\n",
    "#         self.nddata = []\n",
    "#         self.ndlabel = []\n",
    "#         for buck in self.data:\n",
    "#             label = np.empty_like(buck)\n",
    "#             label[:, :-1] = buck[:, 1:]\n",
    "#             label[:, -1] = self.invalid_label\n",
    "#             self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n",
    "#             self.ndlabel.append(ndarray.array(label, dtype=self.dtype))\n",
    "\n",
    "#     def next(self):\n",
    "#         if self.curr_idx == len(self.idx):\n",
    "#             raise StopIteration\n",
    "#         i, j = self.idx[self.curr_idx]\n",
    "#         self.curr_idx += 1\n",
    "\n",
    "#         if self.major_axis == 1:\n",
    "#             data = self.nddata[i][j:j+self.batch_size].T\n",
    "#             label = self.ndlabel[i][j:j+self.batch_size].T\n",
    "#         else:\n",
    "#             data = self.nddata[i][j:j+self.batch_size]\n",
    "#             label = self.ndlabel[i][j:j+self.batch_size]\n",
    "\n",
    "#         return DataBatch([data], [label], pad=0,\n",
    "#                          bucket_key=self.buckets[i],\n",
    "#                          provide_data=[(self.data_name, data.shape)],\n",
    "#                          provide_label=[(self.label_name, label.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i1 = Seq2SeqIterator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src_sent = dataset.src_sent\n",
    "targ_sent = dataset.targ_sent\n",
    "\n",
    "sent_len = lambda x: map(lambda y: len(y), x)\n",
    "max_len = lambda x: max(sent_len(x))\n",
    "min_len = lambda x: min(sent_len(x))\n",
    "\n",
    "min_len = min(min(sent_len(src_sent)), min(sent_len(targ_sent)))\n",
    "# max_len = max(max(sent_len(src_sent)), max(sent_len(targ_sent)))\n",
    "\n",
    "# min_len = min\n",
    "max_len = 65\n",
    "increment = 5\n",
    "\n",
    "all_pairs = [(i, j) for i in range(\n",
    "        min_len,max_len+increment,increment\n",
    "    ) for j in range(\n",
    "        min_len,max_len+increment,increment\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i2 = Seq2SeqIterator(dataset, buckets=all_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
